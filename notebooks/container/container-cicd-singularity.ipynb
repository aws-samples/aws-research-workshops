{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Singularity container using AWS CodePipeline \n",
    "\n",
    "This notebook runs on conda_python3 kernel. \n",
    "\n",
    "In container-basic notebook, we used the command line to create, build and run a few containers on the same instance where we run the notebook. \n",
    "\n",
    "In this notebook, we will build a CICD pipeline to \n",
    "1. Build a docker container image and push it to ECR\n",
    "1. Let the ECR scan for vulnerability in the pushed image\n",
    "1. Manually approve the process after revieing the finding in ECR\n",
    "1. Build the singularity container and push the singularity container image file (sif)  in S3\n",
    "1. Pull down the zip of the sif file from pipeline output artifact folder and unzip it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import project_path # path to helper methods\n",
    "import importlib\n",
    "from lib import workshop\n",
    "from botocore.exceptions import ClientError\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# create a bucket for the workshop to store output files. \n",
    "session = boto3.session.Session()\n",
    "\n",
    "region_name = session.region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "proj_name = 'sigularity-cicd-pipeline'\n",
    "image_tag = 'mySRATools'\n",
    "\n",
    "# we will use this bucket for some artifacts and the output of sratools. \n",
    "bucket = workshop.create_bucket(region_name, session, f\"container-{proj_name}-{account_id}\", False)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# same helper magic for Jupyter to create files more easily\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w+') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate the build process of SRA-Tools container\n",
    "\n",
    "We will re-use the customized container from \"Container Basic\" notebook and continue the genomics use case with the NCBI SRA (Sequence Read Archive) SRA Tool (https://github.com/ncbi/sra-tools) and fasterq-dump (https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump) to extract fastq from SRA-accessions.\n",
    "\n",
    "The command takes a package name as an argument:\n",
    "```\n",
    "$ fasterq-dump SRR000001\n",
    "```\n",
    "\n",
    "We will use the base Ubuntu image and install sra-tools (https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.10.0/sratoolkit.2.10.0-ubuntu64.tar.gz) \n",
    "\n",
    "The workflow of the program in the contianer: \n",
    "1. Upon start, container runs a script \"sratest.sh\".\n",
    "3. sratest.sh will \"prefetch\" the data package, whose name is passed via an environment variable. \n",
    "4. sratest.sh then run \"fasterq-dump\" on the data package\n",
    "5. sratest.sh will then upload the result to S3://{bucket}\n",
    "\n",
    "The output of the fasterq-dump will be stored in s3://{bucket}/data/sra-toolkit/fasterq/{PACKAGE_NAME}\n",
    "\n",
    "## AWS CodePipeline\n",
    "\n",
    "In the \"Container Basics\" notebook we built and ran the container on the same instance that runs the notebook. Here we will use AWS CodePipeline to automate a CI/CD process for our tools.\n",
    "\n",
    "The AWS CodePipeline consists of the following components:\n",
    "\n",
    "1. CodeCommit - code repository, which will contain a \"buildspec.yml\", \"Dockerfile\", and all files needed for the container.\n",
    "2. CodeBuild - this will spawn an instance to run the \"docker build\" and \"push\" the image to Amazon ECR.\n",
    "3. CodeDeploy - we will not use CodeDeploy in this notebook.\n",
    "\n",
    "Each time we checkin code to CodeCommit, it will trigger the entire CodePipeline. When the pipeline finishes, we will have a new version of the docker image in the container registry (Amazon ECR). This allows downstream recipients of our image to take advantage of our work without having to understand the internal details of our container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PACKAGE_NAME='SRR000002'\n",
    "\n",
    "# this is where the output will be stored\n",
    "sra_prefix = 'data/sra-toolkit/fasterq'\n",
    "sra_output = f\"s3://{bucket}/{sra_prefix}\"\n",
    "\n",
    "print(sra_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create the run script\n",
    "This script will be executed via the RUN command in our container. \n",
    "1. It will fetch the sra package by package name\n",
    "2. run fasterq-dump on the package data \n",
    "3. copy the output to S3\n",
    "\n",
    "Note: for teaching purposes, we will be using a different syntax (\"\"\") in this notebook to prepare our files. See Step 4 for the context where we use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sratest_content = \"\"\"#!/bin/bash\n",
    "set -x\n",
    "\n",
    "prefetch $PACKAGE_NAME --output-directory /tmp\n",
    "fasterq-dump $PACKAGE_NAME -e 8\n",
    "aws s3 sync . $SRA_OUTPUT/$PACKAGE_NAME\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Create our own Docker image file\n",
    "\n",
    "Let's build our own image using a Ubuntu base image. \n",
    "\n",
    "1. Install tzdata - as before, we will install this dependency explicitly (with -y argument) to avoid the timezone prompt that would halt the docker build process\n",
    "2. Install wget and awscli.\n",
    "3. Download sratookit ubuntu64 binary and unzip into /opt\n",
    "4. Set the PATH to include the latest sratoolkit/bin\n",
    "5. USER nobody is needed to set the permission for sratookit configuration. \n",
    "6. Use the same sratest.sh script \n",
    "\n",
    "Note: As of Nov 2020, DockerHub has set request limits on their public repos and you might get throttled if you use DockerHub's base image. Therefore, in this example, we will use the base Ubuntu image from AWS public docker registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dockerfile_content=\"\"\"FROM public.ecr.aws/ubuntu/ubuntu:latest\n",
    "\n",
    "RUN apt-get update \n",
    "RUN DEBIAN_FRONTEND=\"noninteractive\" apt-get -y install tzdata \n",
    "RUN apt-get install -y curl wget libxml-libxml-perl awscli uuid-runtime\n",
    "        \n",
    "RUN wget -q https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz -O /tmp/sratoolkit.tar.gz \\\n",
    "        && tar zxf /tmp/sratoolkit.tar.gz -C /opt/ && rm /tmp/sratoolkit.tar.gz && \\\n",
    "        ln -s /opt/sratoolkit.$(curl -s \"https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current.version\")-ubuntu64 /opt/sratoolkit\n",
    "        \n",
    "ENV PATH=\"/opt/sratoolkit/bin/:${PATH}\"\n",
    "ADD sratest.sh /usr/local/bin/sratest.sh\n",
    "RUN chmod +x /usr/local/bin/sratest.sh\n",
    "RUN mkdir /tmp/.ncbi && printf '/LIBS/GUID = \"%s\"\\\\n' `uuidgen` > /tmp/.ncbi/user-settings.mkfg\n",
    "\n",
    "ADD filelist.txt /tmp/filelist.txt\n",
    "ENV HOME=/tmp\n",
    "WORKDIR /tmp\n",
    "USER nobody\n",
    "ENTRYPOINT [\"/usr/local/bin/sratest.sh\"]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Create the build spec file for the docker image build step\n",
    "We will be using the AWS CodeBuild to create our docker image and store it into AWS ECR (private docker image registry). Thus, we need to create a build specifications for this service, such that it knows what commands to run to setup our build environment (pre_build), do the actual build (build section), and then push the image to AWS ECR (post_build). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "buildspec_content =\"\"\"version: 0.2\n",
    "\n",
    "phases:\n",
    "  pre_build:\n",
    "    commands:\n",
    "      - echo Logging in to Amazon ECR...\n",
    "      - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\n",
    "  build:\n",
    "    commands:\n",
    "      - echo Build started on `date`\n",
    "      - echo Building the Docker image...          \n",
    "      - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG -f Dockerfile.cicd . \n",
    "      - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG      \n",
    "  post_build:\n",
    "    commands:\n",
    "      - echo Build completed on `date`\n",
    "      - echo Pushing the Docker image...\n",
    "      - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "\"\"\"\n",
    "\n",
    "#place holder for later use , add in container so we don't have to change the docker file\n",
    "file_list_content = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 5. Create the build spec file for the singularity build step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "buildspec_content_sig =\"\"\"version: 0.2\n",
    "\n",
    "phases:\n",
    "  pre_build:\n",
    "    commands:\n",
    "      - echo Logging in to Amazon ECR...\n",
    "      - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\n",
    "  build:\n",
    "    commands:\n",
    "      - echo Build started on `date`\n",
    "      - echo Install dependencies for singularity container \n",
    "      - yum groupinstall -y 'Development Tools'\n",
    "      - yum install -y epel-release\n",
    "      - yum install -y golang libseccomp-devel squashfs-tools cryptsetup wget git\n",
    "      - wget -O go1.21.3.linux-amd64.tar.gz https://dl.google.com/go/go1.21.3.linux-amd64.tar.gz\n",
    "      - tar -C /usr/local -xzf go1.21.3.linux-amd64.tar.gz\n",
    "      - echo 'export GOPATH=${HOME}/go' >> ~/.bashrc\n",
    "      - echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc\n",
    "      - source ~/.bashrc\n",
    "      - mkdir -p ${GOPATH}/src/github.com/apptainer\n",
    "      - cd ${GOPATH}/src/github.com/apptainer\n",
    "      - git clone --recurse-submodules https://github.com/apptainer/singularity.git\n",
    "      - cd singularity\n",
    "      - git checkout v3.8.7\n",
    "      - ./mconfig \n",
    "      - cd ./builddir\n",
    "      - make\n",
    "      - make install\n",
    "      - SINGULARITY_NOHTTPS=1 singularity build /tmp/$IMAGE_TAG.sif docker://$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "\n",
    "  post_build:\n",
    "    commands:\n",
    "      - echo Sigularity build completed on `date`\n",
    "      - echo Exporting $IMAGE_TAG.sif to artifacts \n",
    "artifacts:\n",
    "  files:\n",
    "    - /tmp/$IMAGE_TAG.sif\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Create an ECR repo\n",
    "Before we can actually build our image, we need to have the repository referenced in our (post_build) phase. We will use boto3 again to interact with the AWS ECR APIs. We will actually use the repository in Step 6,  after the container image is built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecr_client = boto3.client('ecr')\n",
    "try:\n",
    "    resp = ecr_client.create_repository(repositoryName=proj_name)\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'RepositoryAlreadyExistsException':\n",
    "        print(f\"ECR Repo {proj_name} already exists, skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Create an AWS CodeCommit repo and checkin the files\n",
    "\n",
    "We start by setting up the proper access permissions using the IAM service. Each service (CodePipeline, CodeBuild) needs its own policies. We also need to allow these services to access other related services on our behalf (S3 and ECR).\n",
    "\n",
    "Note the CodePipeline and CodeBuild role ARNs that we will use in Step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# roleArn:\n",
    "iam_client = session.client('iam')\n",
    "\n",
    "codepipeline_service_role_name = f\"{proj_name}-codepipeline-service-role\"\n",
    "codepipeline_policies = ['arn:aws:iam::aws:policy/AWSCodePipeline_FullAccess', \n",
    "                         'arn:aws:iam::aws:policy/AWSCodeCommitFullAccess',\n",
    "                         'arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "                         'arn:aws:iam::aws:policy/AWSCodeBuildAdminAccess'\n",
    "                        ]\n",
    "codepipeline_role_arn = workshop.create_service_role_with_policies(codepipeline_service_role_name, 'codepipeline.amazonaws.com', codepipeline_policies )\n",
    "print(codepipeline_role_arn)\n",
    "              \n",
    "codebuild_service_role_name = f\"{proj_name}-codebuild-service-role\"\n",
    "codebuild_policies = ['arn:aws:iam::aws:policy/AWSCodeBuildAdminAccess',\n",
    "                      'arn:aws:iam::aws:policy/CloudWatchFullAccess',\n",
    "                      'arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "                      'arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess']\n",
    "codebuild_role_arn = workshop.create_service_role_with_policies(codebuild_service_role_name, 'codebuild.amazonaws.com', codebuild_policies )\n",
    "print(codebuild_role_arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our files for the initial checkin. We have the Dockerfile, sratest script, buildspec and filelist (empty for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare the files for the checkin\n",
    "put_files=[{\n",
    "               'filePath': 'Dockerfile.cicd',\n",
    "               'fileContent': dockerfile_content\n",
    "            },\n",
    "            {\n",
    "               'filePath': 'sratest.sh',\n",
    "               'fileContent': sratest_content\n",
    "            },\n",
    "            {\n",
    "               'filePath': 'filelist.txt',\n",
    "               'fileContent': file_list_content\n",
    "            },\n",
    "            {\n",
    "               'filePath': 'buildspec.yml',\n",
    "               'fileContent': buildspec_content\n",
    "            },\n",
    "            {\n",
    "               'filePath': 'buildspec_sig.yml',\n",
    "               'fileContent': buildspec_content_sig\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready for the first commit. We will create our code repository and upload our files into the \"main\" branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_in_files(proj_name, put_files):\n",
    "    codecommit_client = boto3.client('codecommit')\n",
    "    try:\n",
    "        resp = codecommit_client.create_repository(repositoryName=proj_name)\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'RepositoryNameExistsException':\n",
    "            print(f\"Repo {proj_name} exists, use that one\")\n",
    "\n",
    "    try:\n",
    "        resp = codecommit_client.get_branch(repositoryName=proj_name, branchName='main')\n",
    "        parent_commit_id = resp['branch']['commitId']\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'BranchDoesNotExistException':\n",
    "            # the repo is new, create it \n",
    "            workshop.commit_files(proj_name, \"main\", put_files,  None)\n",
    "    else:\n",
    "        try:\n",
    "            resp = workshop.commit_files(proj_name, \"main\",put_files, parent_commit_id)\n",
    "        except ClientError as ee:\n",
    "            if ee.response['Error']['Code'] == 'NoChangeException':\n",
    "                print('No change detected. skip commit')\n",
    "                \n",
    "check_in_files(proj_name, put_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Create a CodeBuild project\n",
    "\n",
    "The second stage of the CI/CD pipeline is the build process. We use an instance managed by AWS (see computeType below) to build the container using a standard Amazon Linux 2 build environment. The CodeBuild process is triggered by the CodeCommit code checkins. \n",
    "\n",
    "Note: **codebuild-service-role takes a little longer to propagate**. If you see a permission error, please retry again in a minute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "codebuild_client = boto3.client('codebuild')\n",
    "\n",
    "codebuild_name = f\"build-container\" \n",
    "codecommit_name = f\"Source-{proj_name}\"\n",
    "try: \n",
    "    resp = codebuild_client.create_project(name=codebuild_name, \n",
    "                                       description=\"CICD workshop build demo\",\n",
    "                                       source= {\n",
    "                                           'type': \"CODEPIPELINE\"\n",
    "                                       },\n",
    "                                       artifacts= {\n",
    "                                            \"type\": \"CODEPIPELINE\",\n",
    "                                            \"name\": proj_name\n",
    "                                       },\n",
    "                                       environment= {\n",
    "                                            \"type\": \"LINUX_CONTAINER\",\n",
    "                                            \"image\": \"aws/codebuild/amazonlinux2-x86_64-standard:3.0\",\n",
    "                                            \"computeType\": \"BUILD_GENERAL1_SMALL\",\n",
    "                                            \"environmentVariables\": [\n",
    "                                                {\n",
    "                                                    \"name\": \"AWS_DEFULT_REGION\",\n",
    "                                                    \"value\": region_name,\n",
    "                                                    \"type\": \"PLAINTEXT\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"name\": \"AWS_ACCOUNT_ID\",\n",
    "                                                    \"value\": account_id,\n",
    "                                                    \"type\": \"PLAINTEXT\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"name\": \"IMAGE_REPO_NAME\",\n",
    "                                                    \"value\": proj_name,\n",
    "                                                    \"type\": \"PLAINTEXT\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"name\": \"IMAGE_TAG\",\n",
    "                                                    \"value\": image_tag,\n",
    "                                                    \"type\": \"PLAINTEXT\"\n",
    "                                                }\n",
    "                                            ],\n",
    "                                            \"privilegedMode\": True,\n",
    "                                            \"imagePullCredentialsType\": \"CODEBUILD\"               \n",
    "                                       },\n",
    "                                       logsConfig= {\n",
    "                                                \"cloudWatchLogs\": {\n",
    "                                                    \"status\": \"ENABLED\",\n",
    "                                                    \"groupName\": proj_name\n",
    "                                                },\n",
    "                                                \"s3Logs\": {\n",
    "                                                    \"status\": \"DISABLED\"\n",
    "                                                }\n",
    "                                        },\n",
    "                                        serviceRole= codebuild_role_arn\n",
    "                                      )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ResourceAlreadyExistsException':\n",
    "        print(f\"CodeBuild project {proj_name} exists, skip...\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "\n",
    "print(f\"CodeBuild project name {codebuild_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "codebuild_name_sig = f\"build-singularity\" \n",
    "try: \n",
    "    resp = codebuild_client.create_project(\n",
    "                                       name=codebuild_name_sig, \n",
    "                                       description=\"Build Singularity Container\",\n",
    "                                       source= {\n",
    "                                           'type': \"CODEPIPELINE\",\n",
    "                                           'buildspec': \"buildspec_sig.yml\"\n",
    "                                       },\n",
    "                                       artifacts= {\n",
    "                                            \"type\": \"CODEPIPELINE\",\n",
    "                                            \"name\": proj_name\n",
    "                                       },\n",
    "                                       environment= {\n",
    "                                            \"type\": \"LINUX_CONTAINER\",\n",
    "                                            \"image\": \"aws/codebuild/amazonlinux2-x86_64-standard:3.0\",\n",
    "                                            \"computeType\": \"BUILD_GENERAL1_SMALL\",\n",
    "                                            \"environmentVariables\": [\n",
    "                                                {\n",
    "                                                    \"name\": \"AWS_DEFULT_REGION\",\n",
    "                                                    \"value\": region_name,\n",
    "                                                    \"type\": \"PLAINTEXT\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"name\": \"AWS_ACCOUNT_ID\",\n",
    "                                                    \"value\": account_id,\n",
    "                                                    \"type\": \"PLAINTEXT\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"name\": \"IMAGE_REPO_NAME\",\n",
    "                                                    \"value\": proj_name,\n",
    "                                                    \"type\": \"PLAINTEXT\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"name\": \"IMAGE_TAG\",\n",
    "                                                    \"value\": image_tag,\n",
    "                                                    \"type\": \"PLAINTEXT\"\n",
    "                                                }\n",
    "                                            ],\n",
    "                                            \"privilegedMode\": True,\n",
    "                                            \"imagePullCredentialsType\": \"CODEBUILD\"               \n",
    "                                       },\n",
    "                                       logsConfig= {\n",
    "                                                \"cloudWatchLogs\": {\n",
    "                                                    \"status\": \"ENABLED\",\n",
    "                                                    \"groupName\": proj_name\n",
    "                                                },\n",
    "                                                \"s3Logs\": {\n",
    "                                                    \"status\": \"DISABLED\"\n",
    "                                                }\n",
    "                                        },\n",
    "                                        serviceRole= codebuild_role_arn\n",
    "                                      )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ResourceAlreadyExistsException':\n",
    "        print(f\"CodeBuild project {proj_name} exists, skip...\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "\n",
    "print(f\"CodeBuild project name {codebuild_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. Build the AWS CodePipeline \n",
    "\n",
    "We now combine source, docker build, singularity build steps together into a pipeline with 4 stages commit and build. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "codepipeline_client = boto3.client('codepipeline')\n",
    "\n",
    "stage1 = {\n",
    "    \"name\":f\"{codecommit_name}\",\n",
    "    \"actions\": [\n",
    "        {\n",
    "            \"name\": \"Source\",\n",
    "            \"actionTypeId\": {\n",
    "                \"category\": \"Source\",\n",
    "                \"owner\": \"AWS\",\n",
    "                \"provider\": \"CodeCommit\",\n",
    "                \"version\": \"1\"\n",
    "            },\n",
    "            \"runOrder\": 1,\n",
    "            \"configuration\": {\n",
    "                \"BranchName\": \"main\",\n",
    "                \"OutputArtifactFormat\": \"CODE_ZIP\",\n",
    "                \"PollForSourceChanges\": \"true\",\n",
    "                \"RepositoryName\": proj_name\n",
    "            },\n",
    "            \"outputArtifacts\": [\n",
    "                {\n",
    "                    \"name\": \"SourceArtifact\"\n",
    "                }\n",
    "            ],\n",
    "            \"inputArtifacts\": [],\n",
    "            \"region\": region_name,\n",
    "            \"namespace\": \"SourceVariables\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "stage2 = {\n",
    "   \"name\": f\"{codebuild_name}\",\n",
    "    \"actions\": [\n",
    "        {\n",
    "            \"name\": \"BuildContainer\",\n",
    "            \"actionTypeId\": {\n",
    "                \"category\": \"Build\",\n",
    "                \"owner\": \"AWS\",\n",
    "                \"provider\": \"CodeBuild\",\n",
    "                \"version\": \"1\"\n",
    "            },\n",
    "            \"runOrder\": 1,\n",
    "            \"configuration\": {\n",
    "                \"ProjectName\": codebuild_name\n",
    "            },\n",
    "            \"outputArtifacts\": [\n",
    "                {\n",
    "                    \"name\": \"BuildContainerArtifact\"\n",
    "                }\n",
    "            ],\n",
    "            \"inputArtifacts\": [\n",
    "                {\n",
    "                    \"name\": \"SourceArtifact\"\n",
    "                }\n",
    "            ],\n",
    "            \"region\": region_name,\n",
    "            \"namespace\": \"BuildVariables\"\n",
    "        }\n",
    "    ]    \n",
    "}\n",
    "\n",
    "stage3 = {\n",
    "    \"name\": \"Approval\",\n",
    "    \"actions\": [\n",
    "        {\n",
    "            \"name\": \"ApproveMoveToBuildSingularity\",\n",
    "            \"actionTypeId\": {\n",
    "                \"category\": \"Approval\",\n",
    "                \"owner\": \"AWS\",\n",
    "                \"version\": \"1\",\n",
    "                \"provider\": \"Manual\"\n",
    "            },\n",
    "            \"inputArtifacts\": [],\n",
    "            \"outputArtifacts\": [],\n",
    "            \"configuration\": {\n",
    "            #    \"NotificationArn\": \"arn:aws:sns:us-east-2:80398EXAMPLE:MyApprovalTopic\",\n",
    "                \"ExternalEntityLink\": \"https://us-east-1.console.aws.amazon.com/ecr/repositories?region=us-east-1\",\n",
    "                \"CustomData\": \"Please review scan results in ECR repo\"},\n",
    "            \"runOrder\": 1\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "stage4 = {\n",
    "   \"name\": f\"{codebuild_name_sig}\",\n",
    "    \"actions\": [\n",
    "        {\n",
    "            \"name\": \"BuildSingularity\",\n",
    "            \"actionTypeId\": {\n",
    "                \"category\": \"Build\",\n",
    "                \"owner\": \"AWS\",\n",
    "                \"provider\": \"CodeBuild\",\n",
    "                \"version\": \"1\"\n",
    "            },\n",
    "            \"runOrder\": 1,\n",
    "            \"configuration\": {\n",
    "                \"ProjectName\": codebuild_name_sig\n",
    "            },\n",
    "            \"outputArtifacts\": [\n",
    "                {\n",
    "                    \"name\": \"BuildSingularityArtifact\"\n",
    "                }\n",
    "            ],\n",
    "            \"inputArtifacts\": [\n",
    "                {\n",
    "                    \"name\": \"SourceArtifact\"\n",
    "                }\n",
    "            ],\n",
    "            \"region\": region_name,\n",
    "            \"namespace\": \"BuildSingularityVariables\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "stages = [stage1, stage2, stage3, stage4]\n",
    "\n",
    "\n",
    "pipeline = {\n",
    "    'name': proj_name,\n",
    "    'roleArn': codepipeline_role_arn,\n",
    "    'artifactStore': {\n",
    "        'type': 'S3',\n",
    "        'location': bucket\n",
    "    }, \n",
    "    'stages': stages\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(\"Creating pipeline\")\n",
    "    resp = codepipeline_client.create_pipeline( pipeline= pipeline)\n",
    "    print(\"Created pipeline\",resp)\n",
    "except ClientError as e:\n",
    "    print(e)\n",
    "    if e.response['Error']['Code'] == 'PipelineNameInUseException':\n",
    "        print(f\"Codepipeline {proj_name} already exists \" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "After the pipeline is created, please switch to the Code Pipeline console, you will see that the pipeline is running \n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/codesuite/codepipeline/pipelines/sigularity-cicd-pipeline/view?region=us-east-1\n",
    "\n",
    "When stage 2 is completed, you will need to approve the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Check the container image in the repo\n",
    "\n",
    "Navigate to CodePipeline in the AWS Console to check the status of the step above. The initial CodePipline process will take a few minutes. It will pull assets from CodeCommit, build the docker image on a managed instance, and push the result image into ECR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We should see a container image with the image tag \"mySRATools\" - this is defined as an environment variable in CodeBuild\n",
    "#resp = ecr_client.list_images(repositoryName=proj_name)\n",
    "while True:\n",
    "    resp = ecr_client.describe_images(repositoryName=proj_name)\n",
    "    if resp['imageDetails']:\n",
    "        for image in resp['imageDetails']:\n",
    "            print(\"image pushed at: \" + str(image['imagePushedAt']))\n",
    "        break\n",
    "    else:\n",
    "        clear_output(wait=True)\n",
    "        display(\"Build not done yet, please wait and retry this step. Please do not proceed until you see the 'image pushed' message\")\n",
    "        time.sleep(20)\n",
    "# this is used later in job_definition for AWS Batch\n",
    "image_uri= f\"{account_id}.dkr.ecr.{region_name}.amazonaws.com/{proj_name}:{image_tag}\"\n",
    "print(image_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 9. Now lets update the Dockerfile or the program files\n",
    "\n",
    "just simply add an echo command to the strtest_content script.  The commit step won't run if no files are updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dockerfile_content=\"\"\"FROM public.ecr.aws/ubuntu/ubuntu:latest\n",
    "\n",
    "RUN apt-get update \n",
    "RUN DEBIAN_FRONTEND=\"noninteractive\" apt-get -y install tzdata \n",
    "RUN apt-get install -y curl wget libxml-libxml-perl awscli uuid-runtime\n",
    "        \n",
    "RUN wget -q https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz -O /tmp/sratoolkit.tar.gz \\\n",
    "        && tar zxf /tmp/sratoolkit.tar.gz -C /opt/ && rm /tmp/sratoolkit.tar.gz && \\\n",
    "        ln -s /opt/sratoolkit.$(curl -s \"https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current.version\")-ubuntu64 /opt/sratoolkit\n",
    "        \n",
    "ENV PATH=\"/opt/sratoolkit/bin/:${PATH}\"\n",
    "ADD sratest.sh /usr/local/bin/sratest.sh\n",
    "RUN chmod +x /usr/local/bin/sratest.sh\n",
    "RUN mkdir /tmp/.ncbi && printf '/LIBS/GUID = \"%s\"\\\\n' `uuidgen` > /tmp/.ncbi/user-settings.mkfg\n",
    "\n",
    "ADD filelist.txt /tmp/filelist.txt\n",
    "ENV HOME=/tmp\n",
    "WORKDIR /tmp\n",
    "USER nobody\n",
    "ENTRYPOINT [\"/usr/local/bin/sratest.sh\"]\n",
    "\"\"\"\n",
    "\n",
    "sratest_content = \"\"\"#!/bin/bash\n",
    "set -x\n",
    "\n",
    "prefetch $PACKAGE_NAME --output-directory /tmp\n",
    "fasterq-dump $PACKAGE_NAME -e 8\n",
    "echo 'done - upload files'\n",
    "aws s3 sync . $SRA_OUTPUT/$PACKAGE_NAME\n",
    "\"\"\"\n",
    "\n",
    "put_files=[{\n",
    "               'filePath': 'Dockerfile.cicd',\n",
    "               'fileContent': dockerfile_content\n",
    "            },\n",
    "            {\n",
    "               'filePath': 'sratest.sh',\n",
    "               'fileContent': sratest_content\n",
    "            },\n",
    "            {\n",
    "               'filePath': 'filelist.txt',\n",
    "               'fileContent': file_list_content\n",
    "            },\n",
    "            {\n",
    "               'filePath': 'buildspec.yml',\n",
    "               'fileContent': buildspec_content\n",
    "            },\n",
    "            {\n",
    "               'filePath': 'buildspec_sig.yml',\n",
    "               'fileContent': buildspec_content_sig\n",
    "            }\n",
    "        ]\n",
    "\n",
    "\n",
    "check_in_files(proj_name, put_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the stage2 output \n",
    "\n",
    "In our example, the output artifact is stored as \"/tmp/mySRCTool.sif\" , but the artifact from the build pipeline is actually a zipped file with a shortname \n",
    "\n",
    "when you query the execution details, your will see something like \n",
    "\n",
    "```\n",
    "Container output: {'bucket': 'container-sigularity-cicd-pipeline-xxxxx', 'key': 'sigularity-cicd-pipe/BuildSingu/XASsVIM'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "resp = codepipeline_client.list_action_executions(pipelineName=proj_name)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "\n",
    "if resp['actionExecutionDetails']:\n",
    "    for ae in resp['actionExecutionDetails']:\n",
    "        sig_image_location = ae['output']['outputArtifacts'][0]['s3location']\n",
    "        print(\"Container output: \" + str(sig_image_location))\n",
    "        s3.download_file(sig_image_location['bucket'], sig_image_location['key'], f\"{image_tag}.zip\")\n",
    "        \n",
    "        print(f\"Extract the image into ./tmp/image_tag.sif\") \n",
    "        \n",
    "        with zipfile.ZipFile(f\"{image_tag}.zip\", 'r') as zip_file: \n",
    "            zip_file.extractall(\".\")\n",
    "        #only interested in the latest one\n",
    "        break\n",
    "else:\n",
    "    display(\"No pipeline execution details\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't forget to clean up \n",
    "\n",
    "Only do this if you want to delete your pipeline, and related resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# delete the CI/CD pipeline and repository\n",
    "codepipeline_client.delete_pipeline(name=proj_name)\n",
    "codebuild_client.delete_project(name=codebuild_name)\n",
    "codebuild_client.delete_project(name=codebuild_name_sig)\n",
    "codecommit_client.delete_repository(repositoryName=proj_name)\n",
    "workshop.delete_codecommit_repo(proj_name)\n",
    "workshop.delete_ecr_repo(proj_name)\n",
    "workshop.delete_service_role_with_policies(codepipeline_service_role_name, codepipeline_policies )\n",
    "workshop.delete_service_role_with_policies(codebuild_service_role_name, codebuild_policies )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup S3 bucket\n",
    "workshop.delete_bucket_with_version(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = iam_client.delete_role_policy(RoleName=ROLE_NAME, PolicyName='S3AccessPolicy')\n",
    "except:\n",
    "    print(\"Policy might have been deleted already. Ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
