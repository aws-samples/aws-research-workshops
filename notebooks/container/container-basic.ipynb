{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Container Basics for Research\n",
    "\n",
    "In this workshop, we will start with a simple application running in a Docker container. We will take a closer look at the key components and environments that are needed. \n",
    "\n",
    "There are several container technologies available, but Docker container is the most popular once. We will focus on Docker container in theis workshop. \n",
    "\n",
    "We will also explore diffent ways of running containers in AWS with different services. \n",
    "\n",
    "Why containers for research\n",
    "- Repeatable and sharable tools and applications\n",
    "- Portable - run on different environemnts ( develop on laptop, test on-prem, run large scale in the cloud)\n",
    "- Stackable - run differnet applications in a pipeline with different OS, e.g.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You only need to do this once per kernel - used in analyzing fastq data. If you don't need to run the last step, you don't need this\n",
    "!pip install bioinfokit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import base64\n",
    "import docker\n",
    "import pandas as pd\n",
    "\n",
    "import project_path # path to helper methods\n",
    "from lib import workshop\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# create a bucket for the workshop to store output files. \n",
    "\n",
    "session = boto3.session.Session()\n",
    "bucket = workshop.create_bucket_name('container-ws-')\n",
    "session.resource('s3').create_bucket(Bucket=bucket)\n",
    "\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's create a helper magic for us to easily create and save a file from the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w+') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an application in a container locally.\n",
    "\n",
    "This SageMaker Jupyter notebook runs on an EC2 instance with docker daemon installed. We can build and test docker containers on the same instance. \n",
    "\n",
    "We are going to build a simple web server container that says \"Hello World!\". Let's start with the Docker files \n",
    "\n",
    "### Let's start the the Dockerfile\n",
    "Think about the Dockerfile as the automation script that you usually do on a linux VM. It just run inside an container. You start with a base image (in this case ubuntu:18.04), then you install, configue compile or build the software you need. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate Dockerfile\n",
    "FROM ubuntu:18.04\n",
    "  \n",
    "# Install dependencies and apache web server\n",
    "RUN apt-get update && apt-get -y install apache2\n",
    "\n",
    "# Create the index html\n",
    "RUN echo 'Hello World!' > /var/www/html/index.html\n",
    "\n",
    "# Configure apache \n",
    "RUN echo '. /etc/apache2/envvars' > /root/run_apache.sh && \\\n",
    " echo 'mkdir -p /var/run/apache2' >> /root/run_apache.sh && \\\n",
    " echo 'mkdir -p /var/lock/apache2' >> /root/run_apache.sh && \\\n",
    " echo '/usr/sbin/apache2 -D FOREGROUND' >> /root/run_apache.sh && \\\n",
    " chmod 755 /root/run_apache.sh\n",
    "\n",
    "EXPOSE 80\n",
    "\n",
    "CMD /root/run_apache.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's build the container. \n",
    "\n",
    "The server that runs this SageMaker Jupyter notebook happen to have \"docker\" runtime installed. \n",
    "Docker builld will use the \"Dockerfile\" in the current directory and use \"-t\" to build and tag the image. The image will be in the local docker image registry. \n",
    "\n",
    "We will later learn how to use an external image registry (AWS ECR, e.g.) to push the image to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker build -t simple_server ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the container \n",
    "\n",
    "Run the container locally, we will bind the container port 80 to the localhsot port 8080 (\"-d\" runs detached/background)\n",
    "\n",
    "We use curl to access the web server on port 8080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_id = !docker run  -d -p 8080:80 simple_server\n",
    "    \n",
    "!curl http://localhost:8080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "docker_client = docker.from_env()\n",
    "simple_server_container = docker_client.containers.get(c_id[0])\n",
    "\n",
    "\n",
    "def list_all_running_containers():\n",
    "    docker_client = docker.from_env()\n",
    "    container_list = docker_client.containers.list()\n",
    "    for c in container_list:\n",
    "        print(c.attrs['Id'], c.attrs['State']['Status'])\n",
    "    return container_list\n",
    "\n",
    "running_containers = list_all_running_containers()\n",
    "\n",
    "# Now stop the running container\n",
    "simple_server_container.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run some real workload\n",
    "\n",
    "We are going to use The NCBI SRA (Sequence Read Archive) SRA Tool (https://github.com/ncbi/sra-tools) fasterq-dump (https://github.com/ncbi/sra-tools/wiki/HowTo:-fasterq-dump) to extract fastq from SRA-accessions.\n",
    "\n",
    "The command takes a package name as an argument\n",
    "```\n",
    "$ fasterq-dump SRR000001\n",
    "```\n",
    "\n",
    "The base image is provided by https://hub.docker.com/r/pegi3s/sratoolkit/\n",
    "\n",
    "The workflow of the contianer: \n",
    "1. Upon start, container runs a script \"sratest.sh\".\n",
    "3. sratest.sh will \"prefetch\" the data package, whose name is passed via an environment variable. \n",
    "4. sratest.sh then run fasterq-dump on the dat apackage\n",
    "5. sratest.sh will then upload the result to S3://{bucket}\n",
    "\n",
    "The output of the fasterq-dump will be stored in s3://{bucket}/data/sra-toolkit/fasterq/{{PACKAGE_NAME}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_NAME='SRR000002'\n",
    "\n",
    "# this is where the output will be stored\n",
    "sra_prefix = 'data/sra-toolkit/fasterq'\n",
    "sra_output = f\"s3://{bucket}/{sra_prefix}\"\n",
    "\n",
    "# to run the docker container locally, you need the access credtitials inside the container when usign aws cli\n",
    "# pass the current keys and session token to the container va environment variables\n",
    "credentials = boto3.session.Session().get_credentials()\n",
    "current_credentials = credentials.get_frozen_credentials()    \n",
    "\n",
    "# Please don't print those out:  \n",
    "access_key=current_credentials.access_key\n",
    "secret_key=current_credentials.secret_key\n",
    "token=current_credentials.token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate sratest.sh\n",
    "#!/bin/bash\n",
    "set -x\n",
    "\n",
    "# this is where ncbi/sra-toolkit is installed on the container inside the pegi3s/sratookit image\n",
    "#export PATH=\"/opt/sratoolkit.2.9.6-ubuntu64/bin:${{PATH}}\"\n",
    "prefetch $PACKAGE_NAME --output-directory /tmp\n",
    "fasterq-dump $PACKAGE_NAME -e 18\n",
    "aws s3 sync . $SRA_OUTPUT/$PACKAGE_NAME\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate Dockerfile.pegi3s\n",
    "FROM pegi3s/sratoolkit\n",
    "\n",
    "ENV TZ=America/New_York\n",
    "RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\n",
    "\n",
    "RUN apt-get update --fix-missing && apt-get install -y unzip python3 awscli\n",
    "RUN export PATH=/usr/local/bin/aws/bin:$PATH\n",
    "ADD sratest.sh /usr/local/bin/sratest.sh\n",
    "RUN chmod +x /usr/local/bin/sratest.sh\n",
    "WORKDIR /tmp\n",
    "ENTRYPOINT [\"/usr/local/bin/sratest.sh\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker build -t myncbi/sra-tools -f Dockerfile.pegi3s ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PACKAGE_NAME='SRR000002'\n",
    "\n",
    "# only run this when you need to clean up the registry and storage\n",
    "#!docker system prune -a -f\n",
    "!docker run --env SRA_OUTPUT=$sra_output --env PACKAGE_NAME=$PACKAGE_NAME --env PACKAGE_NAME=$PACKAGE_NAME --env AWS_ACCESS_KEY_ID=$access_key --env AWS_SECRET_ACCESS_KEY=$secret_key --env AWS_SESSION_TOKEN=$token    myncbi/sra-tools:latest\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try a differnet package\n",
    "PACKAGE_NAME = 'SRR000003'\n",
    "!docker run --env SRA_OUTPUT=$sra_output --env PACKAGE_NAME=$PACKAGE_NAME --env PACKAGE_NAME=$PACKAGE_NAME --env AWS_ACCESS_KEY_ID=$access_key --env AWS_SECRET_ACCESS_KEY=$secret_key --env AWS_SESSION_TOKEN=$token    myncbi/sra-tools:latest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your own docker image\n",
    "\n",
    "So far, we have been using existing pegi3s ncbi/sratools image. Let's build our own image using a ubuntu base image. \n",
    "\n",
    "1. Install tzdata - this is a dependency of some of the other packages we need. Normally we do not need to install it specifically, however there is an issue with tzdata requireing an interaction to select timezone during the installation process, which would halt the docker built. so install it separately with -y. \n",
    "2. Install wget and awscli.\n",
    "3. Download sratookit ubuntu binary and unzip into /opt\n",
    "4. set the PATH to include sratoolkit/bin\n",
    "5. USER nobody is needed to set the permission for sratookit configuration. \n",
    "6. use the same sratest.sh script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate Dockerfile.myown\n",
    "#FROM ubuntu:18.04  \n",
    "FROM public.ecr.aws/ubuntu/ubuntu:latest\n",
    "\n",
    "RUN apt-get update \n",
    "\n",
    "RUN DEBIAN_FRONTEND=\"noninteractive\" apt-get -y install tzdata \\\n",
    "        && apt-get install -y wget libxml-libxml-perl awscli\n",
    "\n",
    "RUN wget -q https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.10.0/sratoolkit.2.10.0-ubuntu64.tar.gz -O /tmp/sratoolkit.tar.gz \\\n",
    "        && tar zxf /tmp/sratoolkit.tar.gz -C /opt/ && rm /tmp/sratoolkit.tar.gz\n",
    "\n",
    "ENV PATH=\"/opt/sratoolkit.2.10.0-ubuntu64/bin/:${{PATH}}\"\n",
    "\n",
    "ADD sratest.sh /usr/local/bin/sratest.sh\n",
    "RUN chmod +x /usr/local/bin/sratest.sh\n",
    "WORKDIR /tmp\n",
    "USER nobody\n",
    "ENTRYPOINT [\"/usr/local/bin/sratest.sh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the image\n",
    "!docker build -t myownncbi/sra-tools -f Dockerfile.myown ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_NAME='SRR000004'\n",
    "\n",
    "!docker run --env SRA_OUTPUT=$sra_output --env PACKAGE_NAME=$PACKAGE_NAME --env AWS_ACCESS_KEY_ID=$access_key --env AWS_SECRET_ACCESS_KEY=$secret_key --env AWS_SESSION_TOKEN=$token    myownncbi/sra-tools:latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkou the outfiles on S3\n",
    "s3_client = session.client('s3')\n",
    "objs = s3_client.list_objects(Bucket=bucket, Prefix=sra_prefix)\n",
    "for obj in objs['Contents']:\n",
    "    fn = obj['Key']\n",
    "    p = os.path.dirname(fn)\n",
    "    if not os.path.exists(p):\n",
    "        os.makedirs(p)\n",
    "    s3_client.download_file(bucket, fn , fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# you can use interactive python interpreter, jupyter notebook, google colab, spyder or python code\n",
    "# I am using interactive python interpreter (Python 3.8.2)\n",
    "from bioinfokit.analys import fastq\n",
    "fastq_iter = fastq.fastq_reader(file=f\"{sra_prefix}/{PACKAGE_NAME}/{PACKAGE_NAME}.fastq\") \n",
    "# read fastq file and print out the first 10, \n",
    "i = 0\n",
    "for record in fastq_iter:\n",
    "    # get sequence headers, sequence, and quality values\n",
    "    header_1, sequence, header_2, qual = record\n",
    "    # get sequence length\n",
    "    sequence_len = len(sequence)\n",
    "    # count A bases\n",
    "    a_base = sequence.count('A')\n",
    "    if i < 10:\n",
    "        print(sequence, qual, a_base, sequence_len)\n",
    "    i +=1\n",
    "\n",
    "print(f\"Total number of records for package {PACKAGE_NAME} : {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rb s3://$bucket --force  \n",
    "!rm -rf $sra_prefix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to run the container \n",
    "\n",
    "We looked at creating and running containers locally in this notebook. Please checkout notebook/hpc/hatch-fastqc notebook for running containers in AWS Batch service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
