{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS ParallelCluster Cost Estimation\n",
    "\n",
    "When you create a cluster, all compute resources are tagged with the following tags\n",
    "- ClusterName: name of the cluster\n",
    "- QueueName: name of the queue\n",
    "\n",
    "Those tags will help us identify overall cost of the cluster and the compute nodes of each queue in [AWS Cost and Usage Report (CUR)](https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html). Because those tags are not default cost allocation tags, you will need to enable them from the billing console first. Usually it takes within 24 hours for those cost allocation tags to be activated.\n",
    "\n",
    "However, when you operating a cluster which is shared among different users to run different jobs with different accounts, CUR will not be able to give you the level of breakdown to the job level. \n",
    "\n",
    "In this notebook, we will walk through how you can use the SLURM accounting to allocate costs (estimated) base on jobs, users and accounts (not AWS account , but \"account\" parameter you use when submmiting a slurm job). \n",
    "\n",
    "We will use data from the following sources for cost allocation:\n",
    "- CUR data: using Amazon Athena to query the CUR datalake\n",
    "- SLURM accounting data: using JDBC connections to the MySQL database, which backs the SLURMDBD data store\n",
    "\n",
    "Assumptions:\n",
    "- You have completed pcluster-athena++ notebook and has ran a few simulations or submitted some slurm jobs to the cluster at least one day before. \n",
    "- You have enabled the CUR and created the CUR datalake \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import base64\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import project_path # path to helper methods\n",
    "from lib import workshop\n",
    "from botocore.exceptions import ClientError\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "my_account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# The following 3 parameters are carried over from from pcluster-athena++ notebook. Please change them accordingly\n",
    "# unique name of the pcluster. \n",
    "# This is the name of the cluster - default to \"parallelcluster\" - in this excercise, we default the cluster name to \"parallelcluster\"\n",
    "# If you are using sacctmgr to track accounting for multiple clusters, this could be set differently\n",
    "\n",
    "pcluster_name = 'myPC5c'\n",
    "CLUSTERNAME=\"parallelcluster\"\n",
    "\n",
    "\n",
    "#CLUSTERNAME=\"mypc6g\"\n",
    "#pcluster_name = 'myPC6g'\n",
    "\n",
    "\n",
    "# the rds for the Slurmdbd datastore. We will use a MySQL server as the data store. Server's hostname, username, password will be saved in a secret in Secrets Manager\n",
    "rds_secret_name = 'slurm_dbd_credential'\n",
    "db_name = 'pclusterdb'\n",
    "\n",
    "\n",
    "# the CUR database and table names in the CUR datalake. Please see  AWS Cost and Usage Report (CUR) manual to find out how to set it up\n",
    "# please chagne them to the name of your catalog database and table\n",
    "cur_db = 'athenacurcfn_my_main_cur_in_athena'\n",
    "cur_table = 'my_main_cur_in_athena'\n",
    "\n",
    "# this bucket is used for storing Athena query results\n",
    "bucket_prefix = pcluster_name.lower()+'-accounting-'+my_account_id\n",
    "\n",
    "# use the bucket prefix as name, don't use uuid suffix\n",
    "bucket_name = workshop.create_bucket(region, session, bucket_prefix, False)\n",
    "print(bucket_name)\n",
    "\n",
    "path_name = 'athena'\n",
    "\n",
    "# we will look at the specific month\n",
    "cur_year = '2021'\n",
    "cur_month = '5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### assuem you have created a database secret in SecretManager with the name \"slurm_dbd_credential\"\n",
    "def get_slurm_dbd_rds_secret():\n",
    "    secret_name = rds_secret_name\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret    \n",
    "    \n",
    "# the response is a json {\"username\": \"xxxx\", \"password\": \"xxxx\", \"engine\": \"mysql\", \"host\": \"xxxx\", \"port\": \"xxxx\", \"dbInstanceIdentifier\", \"xxxx\"}\n",
    "rds_secret = json.loads(get_slurm_dbd_rds_secret())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import base64\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "slurm_user = 'slurm'\n",
    "\n",
    "def display_table(data):\n",
    "    html = \"<table>\"\n",
    "    for row in data:\n",
    "        html += \"<tr>\"\n",
    "        for field in row:\n",
    "            html += \"<td><h4>%s</h4><td>\"%(field)\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "###\n",
    "# Retrieve the slurm_token from the SecretManager\n",
    "#\n",
    "def get_secret():\n",
    "    secret_name = \"slurm_token_{}\".format(pcluster_name)\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        print(\"Error\", e)\n",
    "        raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "###\n",
    "# Retrieve the token and inject into the header for JWT auth\n",
    "#\n",
    "def update_header_token():\n",
    "    token = get_secret()\n",
    "    post_headers = {'X-SLURM-USER-NAME':slurm_user, 'X-SLURM-USER-TOKEN': token, 'Content-type': 'application/json', 'Accept': 'application/json'}\n",
    "    get_headers = {'X-SLURM-USER-NAME':slurm_user, 'X-SLURM-USER-TOKEN': token, 'Content-type': 'application/x-www-form-urlencoded', 'Accept': 'application/json'}\n",
    "    return [post_headers, get_headers]\n",
    "\n",
    "###\n",
    "# Convert response into json\n",
    "#\n",
    "def convert_response(resp):\n",
    "    resp_str = resp.content.decode('utf-8')\n",
    "    return json.loads(resp_str)\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "# Print a json array in table format\n",
    "# input: headers [json attribute name, ... ]\n",
    "# input: a - array of json objects\n",
    "def print_table_from_json_array(headers, a):\n",
    "    # add headers as the first row.\n",
    "    t = [headers]\n",
    "    for item in a:\n",
    "        result = []\n",
    "        for h in headers:\n",
    "            result.append(item[h])\n",
    "        t.append(result)\n",
    "    display_table(t)\n",
    "\n",
    "def print_table_from_dict(headers, d):\n",
    "    result = list()\n",
    "    for k,v in d.items():\n",
    "        result.append(v)\n",
    "    print_table_from_json_array(headers, result)\n",
    "        \n",
    "\n",
    "### \n",
    "# wrapper for get\n",
    "#\n",
    "def get_response_as_json(base_url):\n",
    "    _, get_headers = update_header_token()\n",
    "    resp = requests.get(base_url, headers=get_headers)\n",
    "    if resp.status_code != 200:\n",
    "        # This means something went wrong.\n",
    "        print(\"Error\" , resp.status_code)\n",
    "\n",
    "    return convert_response(resp)\n",
    "\n",
    "### \n",
    "# wrapper for post\n",
    "#\n",
    "def post_response_as_json(base_url, data):\n",
    "    post_headers, _ = update_header_token()\n",
    "    resp = requests.post(base_url, headers=post_headers, data=data)\n",
    "    if resp.status_code != 200:\n",
    "        # This means something went wrong.\n",
    "        print(\"Error\" , resp.status_code)\n",
    "\n",
    "    return convert_response(resp)\n",
    "\n",
    "###\n",
    "# Epoch time conversion\n",
    "#\n",
    "def get_localtime(t):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting \n",
    "The Slurm account information is managed by slurmdbd process and stored in a data store( local file or a relational database). In our setup, we use an AWS RDS MySQL database, which has IAM authentication enabled and is running in the save VPC as the ParallelCluster. We can access the database from this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the root ca and install mysql.connector\n",
    "# only need to do this once\n",
    "#\n",
    "\n",
    "#!wget https://s3.amazonaws.com/rds-downloads/rds-ca-2019-root.pem\n",
    "#!pip install mysql.connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector.constants import ClientFlag\n",
    "import sys\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "\n",
    "def get_sacct_as_table_for_cluster(rds_secret, cluster_name):\n",
    "    ENDPOINT= rds_secret['host']\n",
    "    PORT=rds_secret['port']\n",
    "    USER=rds_secret['username']\n",
    "    PASS=rds_secret['password']\n",
    "    DBNAME=\"slurm_acct_db\"\n",
    "\n",
    "\n",
    "    ## use these of your want to use IAM authentication\n",
    "    #os.environ['LIBMYSQL_ENABLE_CLEARTEXT_PLUGIN'] = '1\n",
    "    #session = boto3.Session()\n",
    "    #client = boto3.client('rds')\n",
    "    #token = client.generate_db_auth_token(DBHostname=ENDPOINT, Port=PORT, DBUsername=USR, Region=REGION)\n",
    "\n",
    "    config = {\n",
    "        'user': USER,\n",
    "        'password': PASS,  \n",
    "        'host': ENDPOINT,\n",
    "        'port': PORT,\n",
    "        'database': DBNAME,\n",
    "    # needed for IAM authentication\n",
    "    #    'client_flags': [ClientFlag.SSL],\n",
    "    #    'ssl_ca': 'rds-ca-2019-root.pem',\n",
    "    }\n",
    "\n",
    "    table_headers=['job_db_inx', 'account', 'cpus_req', 'job_name', 'id_job', 'nodelist', 'partition', 'time_submit', 'time_start', 'time_end', 'duration(s)', 'work_dir']\n",
    "    table = []\n",
    "\n",
    "    # partition is a reserved key word in mysql, need to use back tick ` around it\n",
    "    try:\n",
    "        conn =  mysql.connector.connect(**config)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"\"\"SELECT job_db_inx, account, cpus_req, job_name, id_job, nodelist, `partition`, time_submit, time_start, time_end, work_dir from {}_job_table\"\"\".format(cluster_name))\n",
    "        query_results = cur.fetchall()\n",
    "    except Exception as e:\n",
    "        print(\"Database connection failed due to {}\".format(e)) \n",
    "        raise\n",
    "\n",
    "    #job_table_header =[(0,'job_id_inx'), (3, 'account'), ()) \n",
    "    for r in query_results:\n",
    "        l = list(r)\n",
    "        # add a duration before the last element\n",
    "        l.append(l[10])\n",
    "        #duration\n",
    "        l[10] = -1  if ( l[9]==0) else (l[9]-l[8])\n",
    "        l[7] = get_localtime(l[7])\n",
    "        l[8] = get_localtime(l[8])\n",
    "        l[9] = get_localtime(l[9])\n",
    "        table.append(l)\n",
    "    \n",
    "    return table, table_headers\n",
    "\n",
    "    \n",
    "def display_allocated_cost(table, table_headers, daily_cluster_df_cost) :\n",
    "    df = pd.DataFrame(table, columns=table_headers)\n",
    "    # convert time_start from string to datetime\n",
    "\n",
    "    df['time_start'] = pd.to_datetime(df['time_start'])\n",
    "\n",
    "    # drop id_job and job_db_inx - sum of those not useful\n",
    "    df = df.drop(columns=['job_db_inx', 'id_job'])\n",
    "\n",
    "    # sum calculation durations fot account, partition, job_name per day\n",
    "    # partition and time_start is now the index ,need to reset_index to keep the partition, time_start as columns \n",
    "    agg_df= df.groupby(['account', 'partition', 'job_name', df['time_start'].dt.date]).sum().reset_index()\n",
    "\n",
    "    # partition and time_start is now the index \n",
    "    agg_df_daily= agg_df.groupby(['partition', agg_df['time_start']]).sum()\n",
    "\n",
    "    allocations = []\n",
    "    costs = []\n",
    "    costs_total = []\n",
    "    for idx, row in agg_df.iterrows():\n",
    "        loc_idx_queue_datetime = (row['partition'], row['time_start'])\n",
    "        loc_idx_datetime = (row['time_start'])\n",
    "        arow = row['duration(s)']/agg_df_daily.loc[[loc_idx_queue_datetime], 'duration(s)']\n",
    "        try:\n",
    "            row_cost = arow[0]*daily_cluster_df_cost.loc[[loc_idx_datetime], 'compute_cost']\n",
    "            row_total_cost = arow[0]*daily_cluster_df_cost.loc[[loc_idx_datetime], 'cost']\n",
    "        except:\n",
    "            #CUR did not have the queue information in some of the dates. \n",
    "            row_cost = []\n",
    "            row_total_cost = []\n",
    "            row_cost.append(0)\n",
    "            row_total_cost.append(0)\n",
    "\n",
    "        allocations.append(arow[0])\n",
    "        costs.append(row_cost[0])\n",
    "        costs_total.append(row_total_cost[0])\n",
    "\n",
    "    agg_df['allocations'] = allocations\n",
    "    agg_df['compute_cost'] = costs\n",
    "    agg_df['total_cost'] = costs_total\n",
    "    agg_indexed_df =agg_df.set_index(['time_start', 'partition']).sort_values(['time_start', 'partition'])\n",
    "    display(agg_indexed_df)\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the cost allocation for each job\n",
    "\n",
    "Slurm accounting will provide duration in seconds for each account and job_name.  \n",
    "\n",
    "First, call the PClusterCostEstimator to get the daily spending of each queue from the Cost And Usage Report (CUR), using PClusterCostEstimator helper class. This daily cost only include the cost of compute nodes (not the head node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# this is used during developemnt, to reload the module after a change in the module\n",
    "try:\n",
    "    del sys.modules['pcluster_cost_estimator']\n",
    "except:\n",
    "    #ignore if the module is not loaded\n",
    "    print('Module not loaded, ignore')\n",
    "    \n",
    "from pcluster_cost_estimator import PClusterCostEstimator\n",
    "\n",
    "\n",
    "\n",
    "pce = PClusterCostEstimator(cur_db, cur_table, bucket_name, path_name)\n",
    "daily_queue_df = pce.cluster_daily_per_queue_month(pcluster_name, cur_year, cur_month).reset_index()\n",
    "\n",
    "daily_queue_df['time_start'] = pd.to_datetime(daily_queue_df['time_start'])\n",
    "daily_queue_df = daily_queue_df.set_index(['partition','time_start' ])\n",
    "                         \n",
    "#update the index to queue_name+datetime\n",
    "display(daily_queue_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_compute_df = daily_queue_df.groupby(['time_start']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare daily cluster cost with compute cost\n",
    "\n",
    "CUR will provide you with total cost of : \n",
    "- Cluster - with ClusterName = 'my_cluster_name'\n",
    "- Compute cost - with ClusterName = 'my_cluster_name' and QueueName = [queue_names]\n",
    "\n",
    "Compute cost doesn't incldue the cost of the head node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for cn in ['parallelcluster', 'mypc6g']:\n",
    "    print(f\"For cluster {cn}:\")\n",
    "    table, table_headers = get_sacct_as_table_for_cluster(rds_secret, cn)\n",
    "    agg_df = display_allocated_cost(table, table_headers, daily_cluster_df_cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show the total allocated cost by date (add all queue spends together)\n",
    "daily_queue_df_cost = agg_df.groupby(level=0).sum()\n",
    "#display(daily_queue_df_cost)\n",
    "\n",
    "daily_cluster_df_cost = pce.cluster_daily_per_month(pcluster_name, cur_year, cur_month)\n",
    "daily_cluster_df_cost['compute_cost'] = daily_compute_df['cost']\n",
    "display(daily_cluster_df_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workshop.delete_bucket_completely(my_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
