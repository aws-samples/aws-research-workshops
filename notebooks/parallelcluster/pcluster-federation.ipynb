{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slurm Federation on AWS ParallelCluster \n",
    "\n",
    "Built upon what you learning in pcluster-athena++ and pcluster-athena++short notebooks, we will explore how to use Slurm federation on AWS ParallelCluster. \n",
    "\n",
    "Many research institutions have existing on-prem HPC clusters with Slurm scheduler. Those HPC clusters have a fixed size and sometimes require additional capacity to run workloads. \"Bursting into cloud\" is a way to handle that requests. \n",
    "\n",
    "In this notebook, we will\n",
    "1. Build two AWS ParallelClusters - \"awscluster\" (as a worker cluster) and \"onpremcluster\" (to simulate an on-prem cluster)\n",
    "1. Enable REST on \"onpremcluster\"\n",
    "1. Enable Slurm accouting with mySQL as data store on \"onpremcluster\"\n",
    "1. Ebable Slurmdbd on \"awscluster\" to point to the slurm accounting endpoint on \"onpremcluster\"\n",
    "1. Create a federation with \"awscluster\" and \"onpremcluster\" clusters. \n",
    "1. Submit a job from \"onpremcluster\" to \"awscluster\"\n",
    "1. Submit a job from \"awscluster\" to \"onpremcluster\"\n",
    "1. Check job/queue status on both clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import base64\n",
    "import docker\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import project_path # path to helper methods\n",
    "from lib import workshop\n",
    "from botocore.exceptions import ClientError\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "#sys.path.insert(0, '.')\n",
    "import pcluster_athena\n",
    "importlib.reload(pcluster_athena)\n",
    "\n",
    "\n",
    "# unique name of the pcluster\n",
    "onprem_pcluster_name = 'onpremcluster'\n",
    "onprem_config_name = \"config-simple\"\n",
    "onprem_post_install_script_prefix = \"scripts/pcluster_post_install_onprem.sh\"\n",
    "\n",
    "# unique name of the pcluster\n",
    "aws_pcluster_name = 'awscluster'\n",
    "aws_config_name = \"config-simple\"\n",
    "aws_post_install_script_prefix = \"scripts/pcluster_post_install_aws.sh\"\n",
    "\n",
    "federation_name = \"burstworkshop\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used during developemnt, to reload the module after a change in the module\n",
    "try:\n",
    "    del sys.modules['pcluster_athena']\n",
    "except:\n",
    "    #ignore if the module is not loaded\n",
    "    print('Module not loaded, ignore')\n",
    "    \n",
    "from pcluster_athena import PClusterHelper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the onprem clsuter\n",
    "onprem_pcluster_helper = PClusterHelper(onprem_pcluster_name, onprem_config_name, onprem_post_install_script_prefix, federation_name=federation_name)\n",
    "onprem_pcluster_helper.create_before()\n",
    "!pcluster create $onprem_pcluster_helper.pcluster_name -nr -c build/$onprem_config_name\n",
    "onprem_pcluster_helper.create_after()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use onprem cluster headnode as the dbd server. \n",
    "\n",
    "onprem_pcluster_status = !pcluster status $onprem_pcluster_name\n",
    "# Grab the IP of the head node, on where Slurm REST endpoint runs. The returned IP is a private IP of the head node. Make sure your SageMaker notebook\n",
    "# is created in the same VPC (default VPC)\n",
    "dbd_host = onprem_pcluster_status.grep('MasterPrivateIP').s.split()[1]\n",
    "\n",
    "print(dbd_host)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the onprem clsuter\n",
    "aws_pcluster_helper = PClusterHelper(aws_pcluster_name, aws_config_name, aws_post_install_script_prefix, dbd_host=dbd_host, federation_name=federation_name)\n",
    "aws_pcluster_helper.create_before()\n",
    "!pcluster create $aws_pcluster_helper.pcluster_name -nr -c build/$aws_config_name\n",
    "aws_pcluster_helper.create_after()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add security group to each cluster security group - this only applies to the current configuration where \n",
    "# both clusters are in AWS. \n",
    "# For a real on-prem environment, you will need to configure your network firewall to allow traffic between the two clusters\n",
    "# Each pcluster is created with a set of cloudformation templates. We can get some detailed information from the stack\n",
    "#!pcluster status $aws_pcluster_name\n",
    "\n",
    "cf_client = boto3.client(\"cloudformation\")\n",
    "aws_pcluster_head_sg = cf_client.describe_stack_resource(StackName='parallelcluster-'+aws_pcluster_name, LogicalResourceId='MasterSecurityGroup')['StackResourceDetail']['PhysicalResourceId']\n",
    "onprem_pcluster_head_sg = cf_client.describe_stack_resource(StackName='parallelcluster-'+onprem_pcluster_name, LogicalResourceId='MasterSecurityGroup')['StackResourceDetail']['PhysicalResourceId']\n",
    "\n",
    "print(aws_pcluster_head_sg)\n",
    "print(onprem_pcluster_head_sg)\n",
    "\n",
    "ec2_client = boto3.client(\"ec2\")\n",
    "try:\n",
    "    resp = ec2_client.authorize_security_group_ingress(GroupId=aws_pcluster_head_sg , IpPermissions=[ {'FromPort': -1, 'IpProtocol': '-1', 'UserIdGroupPairs': [{'GroupId': onprem_pcluster_head_sg}] } ] ) \n",
    "except ClientError  as err:\n",
    "    print(err , \" this is ok , we can ignore\")\n",
    "\n",
    "try:\n",
    "    resp = ec2_client.authorize_security_group_ingress(GroupId=onprem_pcluster_head_sg , IpPermissions=[ {'FromPort': -1, 'IpProtocol': '-1', 'UserIdGroupPairs': [{'GroupId': aws_pcluster_head_sg}] } ] ) \n",
    "except ClientError  as err:\n",
    "    print(err , \" this is ok , we can ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add awscluster to the federation. \n",
    "\n",
    "After two clusters are created and security groups attached. run the following command on awscluster headnode\n",
    "```\n",
    "su -c \"/opt/slurm/bin/sacctmgr -i add cluster awscluster\" slurm\n",
    "su -c \"/opt/slurm/bin/sacctmgr -i modify federation curstworkshop Clusters+=awscluster\" slurm\n",
    "\n",
    "# restart slurmctd  - this needs to be done after slurmdbd start, otherwise the cluster won't register\n",
    "systemctl restart slurmctld\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate with Slurm REST API running on the head node\n",
    "\n",
    "SLURM REST is currently running on the headnode. The JWT token is stored in AWS Secret Manager from the head node. You will need that JWT token in the header of all your REST API requests. \n",
    "\n",
    "Don't forget to add secretsmanager:GetSecretValue permission to the sagemaker execution role that runs this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Slurm REST API Schema\n",
    "\n",
    "We will start by examing the Slurm REST API schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "slurm_host = dbd_host\n",
    "slurm_openapi_ep = 'http://'+slurm_host+':8082/openapi/v3'\n",
    "slurm_rest_base='http://'+slurm_host+':8082/slurm/v0.0.35'\n",
    "\n",
    "_, get_headers = onprem_pcluster_helper.update_header_token()\n",
    "\n",
    "resp_api = requests.get(slurm_openapi_ep, headers=get_headers)\n",
    "print(resp_api)\n",
    "\n",
    "if resp_api.status_code != 200:\n",
    "    # This means something went wrong.\n",
    "    print(\"Error\" , resp_api.status_code)\n",
    "\n",
    "with open('build/slurm_api.json', 'w') as outfile:\n",
    "    json.dump(resp_api.json(), outfile)\n",
    "\n",
    "print(json.dumps(resp_api.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use REST API callls to interact with ParallelCluster\n",
    "\n",
    "Then we will make direct REST API requests to retrieve the partitions in response\n",
    "\n",
    "If you get server errors, you can login to the head-node and check the system logs of \"slurmrestd\", which is running as a service. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partition_info = [\"name\", \"nodes\", \"nodes_online\", \"total_cpus\", \"total_nodes\"]\n",
    "\n",
    "##### This works as well, \n",
    "# update header in case the token has expired\n",
    "_, get_headers = onprem_pcluster_helper.update_header_token()\n",
    "\n",
    "##### call REST API directly\n",
    "slurm_partitions_url= slurm_rest_base+'/partitions/'\n",
    "partitions = onprem_pcluster_helper.get_response_as_json(slurm_partitions_url)\n",
    "#print(partitions['partitions'])\n",
    "#20.02.4 returns a dict, not an array\n",
    "onprem_pcluster_helper.print_table_from_dict(partition_info, partitions['partitions'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a job\n",
    "The slurm_rest_api_client job submit function doesn't include the \"script\" parameter. We will have to use the REST API Post directly. \n",
    "\n",
    "The body of the post should be like this.  \n",
    "\n",
    "```\n",
    "{\"job\": {\"account\": \"test\", \"ntasks\": 20, \"name\": \"test18.1\", \"nodes\": [2, 4],\n",
    "\"current_working_directory\": \"/tmp/\", \"environment\": {\"PATH\": \"/bin:/usr/bin/:/usr/local/bin/\",\"LD_LIBRARY_PATH\":\n",
    "\"/lib/:/lib64/:/usr/local/lib\"} }, \"script\": \"#!/bin/bash\\necho it works\"}\n",
    "```\n",
    "When the job is submitted through REST API, it will run as the user \"slurm\". That's what the work directory \"/shared/tmp\" should be owned by \"slurm:slurm\", which is done in the post_install script. \n",
    "\n",
    "fetch_and_run.sh will fetch the sbatch script and the input file from S3 and put them in /shared/tmp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program batch script, input and output files\n",
    "\n",
    "Within the installation, we included a pre-installed athena++ package with an input file under /shared/tmp. We will use that to submit a job form onpremcluster to awscluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_script = \"#!/bin/bash\\ncd /shared/tmp/orszag-tang-lowres\\nsbatch -Mawscluster batch_athena.sh\"\n",
    "job_script = \"/shared/tmp/batch_test.sh\"\n",
    "\n",
    "slurm_job_submit_base=slurm_rest_base+'/job/submit'\n",
    "\n",
    "#in order to use Slurm REST to submit jobs, you need to have the working directory permission set to nobody:nobody. in this case /efs/tmp\n",
    "data = {'job':{ 'account': 'testaccount', 'partition': 'q1', 'name': 'federation_test', 'current_working_directory':'/shared/tmp/', 'environment': {\"PATH\": \"/bin:/usr/bin/:/usr/local/bin/:/opt/slurm/bin:/opt/amazon/openmpi/bin\",\"LD_LIBRARY_PATH\":\n",
    "\"/lib/:/lib64/:/usr/local/lib:/opt/slurm/lib:/opt/slurm/lib64\"}}, 'script':job_script}\n",
    "\n",
    "###\n",
    "# This job submission will generate two jobs , the job_id returned in the response is for the bash job itself. the sbatch will be the job_id+1 run subsequently.\n",
    "#\n",
    "resp_job_submit = onprem_pcluster_helper.post_response_as_json(slurm_job_submit_base, data=json.dumps(data))\n",
    "\n",
    "\n",
    "print(resp_job_submit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List recent jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all the jobs immediately after the previous step. This should return two running jobs. \n",
    "slurm_jobs_base=slurm_rest_base+'/jobs'\n",
    "\n",
    "jobs = onprem_pcluster_helper.get_response_as_json(slurm_jobs_base)\n",
    "# print(jobs)\n",
    "jobs_headers = [ 'job_id', 'job_state', 'account', 'batch_host', 'nodes', 'cluster', 'partition', 'current_working_directory']\n",
    "\n",
    "# newer version of slurm \n",
    "#print_table_from_json_array(jobs_headers, jobs['jobs'])\n",
    "onprem_pcluster_helper.print_table_from_json_array(jobs_headers, jobs)\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mediu resolution simulation will run about ten minutes, plus the time for the cluster to spin up. Wait till the job finishes running then move to the next sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't forget to clean up\n",
    "\n",
    "1. Delete the ParallelCluster\n",
    "2. Delete the RDS\n",
    "3. S3 bucket\n",
    "4. Secrets used in this excercise\n",
    "\n",
    "Deleting VPC is risky, I will leave it out for you to manually clean it up if you created a new VPC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used during developemnt, to reload the module after a change in the module\n",
    "#try:\n",
    "#    del sys.modules['pcluster_athena']\n",
    "#except:\n",
    "#    #ignore if the module is not loaded\n",
    "#    print('Module not loaded, ignore')\n",
    "    \n",
    "#from pcluster_athena import PClusterHelper\n",
    "# we added those ingress rules later, if we don't remove them, pcluster delete will fail\n",
    "try:\n",
    "    resp = ec2_client.revoke_security_group_ingress(GroupId=aws_pcluster_head_sg , IpPermissions=[ {'FromPort': -1, 'IpProtocol': '-1', 'UserIdGroupPairs': [{'GroupId': onprem_pcluster_head_sg}] } ] ) \n",
    "except ClientError  as err:\n",
    "    print(err , \" this is ok , we can ignore\")\n",
    "\n",
    "try:\n",
    "    resp = ec2_client.revoke_security_group_ingress(GroupId=onprem_pcluster_head_sg , IpPermissions=[ {'FromPort': -1, 'IpProtocol': '-1', 'UserIdGroupPairs': [{'GroupId': aws_pcluster_head_sg}] } ] ) \n",
    "except ClientError  as err:\n",
    "    print(err , \" this is ok , we can ignore\")\n",
    "    \n",
    "    \n",
    "aws_pcluster_helper = PClusterHelper(aws_pcluster_name, aws_config_name, aws_post_install_script_prefix)\n",
    "!pcluster delete $aws_pcluster_helper.pcluster_name\n",
    "aws_pcluster_helper.cleanup_after(KeepRDS=True)\n",
    "\n",
    "onprem_pcluster_helper = PClusterHelper(onprem_pcluster_name, onprem_config_name, onprem_post_install_script_prefix)\n",
    "!pcluster delete $onprem_pcluster_helper.pcluster_name\n",
    "onprem_pcluster_helper.cleanup_after(KeepRDS=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
