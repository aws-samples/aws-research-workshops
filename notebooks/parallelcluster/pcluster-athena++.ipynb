{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AWS ParallelCluster\n",
    "\n",
    "## Create a cluster\n",
    "\n",
    "\n",
    "Before you start, please have the following pre-requisites ready. \n",
    "* A VPC that has a public subnet with an internet gateway\n",
    "* A MySQL RDS database in the same subnet ( or you can use this notebook to create one )\n",
    "* The SageMaker execution role used for this notebook have permission to create a ParallelCluster, create EC2 keypair, secrets in AWS SecretManager and VPCs \n",
    "\n",
    "Details about the policies are described in this document. \n",
    "https://docs.aws.amazon.com/parallelcluster/latest/ug/iam.html#parallelclusteruserpolicy\n",
    "\n",
    "As an alternative, you can create a IAM user that has the policies mentioned above, and add the aws_access_key_id and aws_secret_access_key in the [aws] section of the following config file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import base64\n",
    "import docker\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import project_path # path to helper methods\n",
    "from lib import workshop\n",
    "from botocore.exceptions import ClientError\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "import pcluster_athena\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "ec2_client = boto3.client('ec2')\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "# Get the aws account number where this notebook, the cluster sits. It's used in IAM policy resource\n",
    "my_account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# specify the following names\n",
    "\n",
    "# ssh key for access the pcluster. this key is not needed  in this excercise, but useful if you need to ssh into the headnode of the pcluster\n",
    "key_name = 'pcluster-athena-key'\n",
    "keypair_saved_path = './'+key_name+'.pem'\n",
    "# unique name of the pcluster\n",
    "pcluster_name = 'myTestCluster6g'\n",
    "config_name = \"config-6g\"\n",
    "pcluster_ = 'scripts/post_install_script-6g.sh'\n",
    "\n",
    "\n",
    "# the rds for the Slurmdbd datastore. We will use a MySQL server as the data store. Server's hostname, username, password will be saved in a secret in Secrets Manager\n",
    "rds_secret_name = 'slurm_dbd_credential'\n",
    "# the slurm REST token is generated from the headnode and stored in Secrets Manager. This token is used in makeing REST API calls to the Slurm REST endpoint running on the headnode \n",
    "slurm_secret_name = \"slurm_token_{}\".format(pcluster_name)\n",
    "# database name for the slurmdbd data store in MySQL database. \n",
    "db_name = 'pclusterdb'\n",
    "# We only need one subnet for the pcluster, but two subnets are needed for RDS instance. If use existing VPC, we will use the default VPC, and the first subnet in default VPC\n",
    "use_existing_vpc = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During development, everytime you update the workshop module, you need to call this:  \n",
    "importlib.reload(workshop)\n",
    "\n",
    "# we will not need to use the ssh_key in this excercise. However, you can only download the key once during creation. we will save it in case\n",
    "try:\n",
    "    workshop.create_keypair(region, session, key_name, keypair_saved_path)\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"InvalidKeyPair.Duplicate\":\n",
    "        print(\"KeyPair with the name {} alread exists. Skip\".format(key_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VPC\n",
    "\n",
    "You can use the existing default VPC or create a new VPC with 2 subnets. \n",
    "\n",
    "We will only be using one of the subnets for the ParallelCluster, but both are used for the RDS database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if use_existing_vpc:\n",
    "    vpc_filter = [{'Name':'isDefault', 'Values':['true']}]\n",
    "    default_vpc = ec2_client.describe_vpcs(Filters=vpc_filter)\n",
    "    vpc_id = default_vpc['Vpcs'][0]['VpcId']\n",
    "\n",
    "    subnet_filter = [{'Name':'vpc-id', 'Values':[vpc_id]}]\n",
    "    subnets = ec2_client.describe_subnets(Filters=subnet_filter)\n",
    "    subnet_id = subnets['Subnets'][0]['SubnetId']\n",
    "    subnet_id2 = subnets['Subnets'][1]['SubnetId']    \n",
    "else: \n",
    "    vpc, subnet1, subnet2 = workshop.create_and_configure_vpc()\n",
    "    vpc_id = vpc.id\n",
    "    subnet_id = subnet1.id\n",
    "    subnet_id2 = subnet2.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the project bucket. \n",
    "# we will use this bucket for the scripts, input and output files \n",
    "\n",
    "\n",
    "bucket_prefix = pcluster_name.lower()+'-'+my_account_id\n",
    "\n",
    "# use the bucket prefix as name, don't use uuid suffix\n",
    "my_bucket_name = workshop.create_bucket(region, session, bucket_prefix, False)\n",
    "print(my_bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SPOT Instances\n",
    "We will create two queues in this excercise, one using on-demand instances and one using SPOT instances. To use SPOT, we need AWSServiceRoleForEC2SpotFleet service-linked role in this account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    iam_client.get_role(RoleName=\"AWSServiceRoleForEC2SpotFleet\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "        print(\"AWSServiceRoleForEC2SpotFleet doesn't exist, create one ... \")\n",
    "        iam_client.create_service_linked_role(AWSServiceName='spotfleet.amazonaws.com')\n",
    "        print(\"AWSServiceRoleForEC2SpotFleet created successfully\")\n",
    "else: \n",
    "    print(\"AWSServiceRoleForEC2SpotFleet exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDS Database (MySQL) - used with ParallelCluster for accounting\n",
    "\n",
    "We will create a simple MySQL RDS database instance to use as a data store for Slurmdbd for accounting. The username and password are stored as a secret in the Secrets Manager. \n",
    "The secret is later used to configure Slurmdbd. \n",
    "\n",
    "The RDS instance will be created asynchronuously. While the secret is created immediated, the hostname will be available only after the creation is completed. We will have to update the hostname in the secreat afterwards. \n",
    "\n",
    "We will update the security group to allow traffic to port 3306 from the cluster in the same vpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RDS for cluster accounting\n",
    "importlib.reload(workshop)\n",
    "\n",
    "\n",
    "# create a simple mysql rds instance , the username and password will be stored in secrets maanger as a secret\n",
    "workshop.create_simple_mysql_rds(region, session, db_name, [subnet_id,subnet_id2] ,rds_secret_name)\n",
    "\n",
    "rds_client = session.client('rds', region)\n",
    "rds_waiter = rds_client.get_waiter('db_instance_available')\n",
    "\n",
    "print(\"Waiting for the DB creation to finish ... \")\n",
    "try:\n",
    "    rds_waiter.wait(DBInstanceIdentifier=db_name) \n",
    "except botocore.exceptions.WaiterError as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"Finished creating the db.\")\n",
    "\n",
    "#since the rds creation is asynch, need to wait till the creation is done to get the hostname, then update the secret with the hostname\n",
    "vpc_sgs = workshop.get_sgs_and_update_secret(region, session, db_name, rds_secret_name)\n",
    "print(vpc_sgs)\n",
    "\n",
    "# Step 3. get the vpc local CIDR range \n",
    "ec2 = boto3.resource('ec2')\n",
    "vpc = ec2.Vpc(vpc_id)\n",
    "cidr = vpc.cidr_block\n",
    "\n",
    "# update the RDS security group to allow inbound traffic to port 3306\n",
    "workshop.update_security_group(vpc_sgs[0]['VpcSecurityGroupId'], cidr, 3306)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vpc_sgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pcluster CLI\n",
    "\n",
    "If you have not installed aws-parallelcluster commandline tool, uncomment the next line of code and executed it. You only need to do it once. \n",
    "\n",
    "If you have installed \"pcluster\" command correctly, it should return \"2.10.4\"\n",
    "\n",
    "Note: You only need to do this once in this kernel. If you have not installed pcluster, uncomment the next two lines and run the block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install --upgrade pip\n",
    "#!sudo pip3 install --upgrade aws-parallelcluster\n",
    "!pcluster version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParallelCluster config file\n",
    "Start with the the configuration template file \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[aws]\n",
      "aws_region_name = ${REGION}\n",
      "\n",
      "[vpc public]\n",
      "vpc_id = ${VPC_ID}\n",
      "master_subnet_id = ${SUBNET_ID}\n",
      "\n",
      "[cluster default]\n",
      "key_name = ${KEY_NAME}\n",
      "base_os = alinux2\n",
      "scheduler = slurm\n",
      "master_instance_type = c5.xlarge\n",
      "s3_read_write_resource = *\n",
      "vpc_settings = public\n",
      "ebs_settings = myebs\n",
      "queue_settings = q1, q2, q3\n",
      "post_install = ${POST_INSTALL_SCRIPT_LOCATION}\n",
      "post_install_args = ${POST_INSTALL_SCRIPT_ARGS}\n",
      "additional_iam_policies = arn:aws:iam::aws:policy/SecretsManagerReadWrite\n",
      "\n",
      "[queue q1]\n",
      "compute_resource_settings = cr1\n",
      "placement_group = DYNAMIC\n",
      "enable_efa = true\n",
      "disable_hyperthreading = true\n",
      "compute_type = ondemand\n",
      "\n",
      "[queue q2]\n",
      "compute_resource_settings = cr2\n",
      "placement_group = DYNAMIC\n",
      "enable_efa = false\n",
      "disable_hyperthreading = false\n",
      "compute_type = spot\n",
      "\n",
      "[compute_resource cr1]\n",
      "instance_type = c5n.18xlarge\n",
      "min_count = 0\n",
      "initial_count = 0\n",
      "max_count = 20\n",
      "\n",
      "[compute_resource cr2]\n",
      "instance_type = c5n.2xlarge\n",
      "min_count = 0\n",
      "initial_count = 0\n",
      "max_count = 10\n",
      "\n",
      "[ebs myebs]\n",
      "shared_dir = /shared\n",
      "volume_type = gp2\n",
      "volume_size = 200\n",
      "\n",
      "[aliases]\n",
      "ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS}\n"
     ]
    }
   ],
   "source": [
    "config_file_name=config_name+'.ini'\n",
    "\n",
    "!cat config/$config_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup parameters for PCluster\n",
    "\n",
    "We will be using a relational database on AWS (RDS) for Slurm accounting (slurmdbd). Please refer to this blog for how to set it up https://aws.amazon.com/blogs/compute/enabling-job-accounting-for-hpc-with-aws-parallelcluster-and-amazon-rds/\n",
    "\n",
    "Once you set up the MySQL RDS, create a secret in SecretManager with the type \"Credentials for RDS\", so we don't need to expose the database username/password in plain text in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is used during developemnt, to reload the module after a change in the module\n",
    "try:\n",
    "    del sys.modules['pcluster_athena']\n",
    "except:\n",
    "    #ignore if the module is not loaded\n",
    "    print('Module not loaded, ignore')\n",
    "    \n",
    "from pcluster_athena import PClusterHelper\n",
    "# create the cluster - # You can rerun the rest of the notebook again with no harm. There are checks in place for existing resoources. \n",
    "pcluster_helper = PClusterHelper(pcluster_name, config_name, post_install_script_prefix)\n",
    "\n",
    "    \n",
    "    \n",
    "# the response is a json {\"username\": \"xxxx\", \"password\": \"xxxx\", \"engine\": \"mysql\", \"host\": \"xxxx\", \"port\": \"xxxx\", \"dbInstanceIdentifier\", \"xxxx\"}\n",
    "rds_secret = json.loads(pcluster_helper.get_slurm_dbd_rds_secret())\n",
    "\n",
    "post_install_script_location = \"s3://{}/{}\".format(pcluster_helper.my_bucket_name, post_install_script_prefix)\n",
    "post_install_script_args = \"'\" + rds_secret['host']+' '+str(rds_secret['port']) +' ' + rds_secret['username'] + ' ' + rds_secret['password'] + ' ' + pcluster_name + ' ' + region +\"'\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post installation script\n",
    "This script is used to recompile and configure slurm with slurmrestd. We also added the automation of compiling Athena++ in the script. \n",
    "\n",
    "Let's take a look at the scrupt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat scripts/pcluster_post_install.sh\n",
    "\n",
    "#upload the script to S3\n",
    "session = boto3.Session()\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "try:\n",
    "    resp = s3_client.upload_file('scripts/pcluster_post_install.sh', my_bucket_name, post_install_script_prefix)\n",
    "except ClientError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the placeholder with value in config.ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = {'${REGION}': region, \n",
    "      '${VPC_ID}': vpc_id, \n",
    "      '${SUBNET_ID}': subnet_id, \n",
    "      '${KEY_NAME}': key_name, \n",
    "      '${POST_INSTALL_SCRIPT_LOCATION}': post_install_script_location, \n",
    "      '${POST_INSTALL_SCRIPT_ARGS}': post_install_script_args\n",
    "     }\n",
    "\n",
    "\n",
    "!mkdir -p build\n",
    "pcluster_helper.template_to_file(\"config/\"+config_name+\".ini\", \"build/\"+config_name\", ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat build/config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pcluster with the config file\n",
    "\n",
    "The -nr note is used to tell cloudformation not to roll back when there is an error - this is only needed for development. \n",
    "\n",
    "After the cluster is created, we will use boto to setup the following permissions\n",
    "1. Add IAM permission on the head-node instance role to allow access to Secret Manager for storing slurm token \n",
    "2. Add Inbound rule to allow \"All traffic\" from the SageMaker notebook instance (for Slurmrest API access)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pcluster create $pcluster_name -nr -c build/$config_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pcluster list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update IAM policy and security group \n",
    "\n",
    "Use boto3 to \n",
    "1. Update a policy in parallelcluster head-node instance role, to allow the head-node to access Secret Manager.\n",
    "2. Add inbound rule to allow access to the REST API from this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the lib during development\n",
    "importlib.reload(workshop)\n",
    "# Use the stack name to find the resources created with the parallelcluster. Use some of the information to update\n",
    "# the IAM policy and security group\n",
    "cluster_stack_name = 'parallelcluster-'+pcluster_name\n",
    "\n",
    "\n",
    "#SGet the head-node's instanace role and headnode security group \n",
    "cf_client = boto3.client('cloudformation')\n",
    "root_role_info = cf_client.describe_stack_resource(StackName=cluster_stack_name, LogicalResourceId='RootRole' )\n",
    "sg_info = cf_client.describe_stack_resource(StackName=cluster_stack_name, LogicalResourceId='MasterSecurityGroup' )\n",
    "\n",
    "#Root role  and security group physical resource id\n",
    "root_role_name = root_role_info['StackResourceDetail']['PhysicalResourceId']\n",
    "head_sg_name = sg_info['StackResourceDetail']['PhysicalResourceId']\n",
    "\n",
    "# To put the head/compute nodes under managed instances in SSM, attach AmazonSSMManagedInstanceCore policy to the root_role\n",
    "# Note - if you enable this, patch schedule might interrupt your computation if the scheduled patch happens during the computation\n",
    "iam_client.attach_role_policy(RoleName=root_role_name, PolicyArn='arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore')\n",
    "\n",
    "# Step 3. get the vpc local CIDR range \n",
    "ec2 = boto3.resource('ec2')\n",
    "vpc = ec2.Vpc(vpc_id)\n",
    "cidr = vpc.cidr_block\n",
    "\n",
    "workshop.update_security_group(head_sg_name, cidr, 8082)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the pcluster head-node PrivateIP \n",
    "# ! cmd returns a IPython.utils.text.SList, which has grep, fields methods and s,n,p properties\n",
    "####\n",
    "pcluster_status = !pcluster status $pcluster_name\n",
    "\n",
    "# get the second part of 'MasterPrivateIP: 172.16.2.92'\n",
    "slurm_host = pcluster_status.grep('MasterPrivateIP').s.split()[1]\n",
    "\n",
    "print(slurm_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate with Slurm REST API running on the head node\n",
    "\n",
    "\n",
    "![Slurmrestd_diagram](parallelcluster_restd_diagram.png \"Slurm REST API on AWS ParallelCluster\")\n",
    "\n",
    "Slurmrestd is currently running on the headnode, using jwt as the auth mechanism. \n",
    "In the post_install script, slurmrestd is enabled to run as a daemon with the following command on the head-node. \n",
    "\n",
    "We will be using direct REST API calls with JWT token (retrieved from Secret Manager) in the header. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Integrate with Slurm REST API running on the head node\n",
    "\n",
    "SLURM REST is currently running on the headnode, using jwt as the auth mechanism. On the server side, a JWT token is created every 20 minutes by running ```scontrol token username=slurm``` on the head node. The same token is needed in the HTTP header with every GET/POST requrests from the notebook. We use AWS Secret Manager to store the encrypted token so it can be accessed from the notebook. \n",
    "\n",
    "### JWT token\n",
    "\n",
    "To pass it securely to this notebook, we will first create a cron job on the headnode to retrieve the token, then save it in Secrete Manager with a name \"slurm_token_{cluster_name}\". The default JWT token lifespan is 1800 seconds(30 mins). Run the follow script on the head-node as a cron job to update the token every 20 mins\n",
    "\n",
    "The following steps are included in the post_install_script. You DO NOT need to run it. \n",
    "#### Step 1.  Add permission to the instance role for the head-node\n",
    "We use additional_iam_role in the pcluster configuration file to attach SecretManager read/write policy to the instance role on the cluster. \n",
    "\n",
    "#### Step 2. Create a script \"token_refresher.sh\" \n",
    "Assume we save the following script at /shared/token_refresher.sh \n",
    "\n",
    "``` token_refresher.sh\n",
    "#!/bin/bash\n",
    "\n",
    "REGION=us-east-1\n",
    "export $(/opt/slurm/bin/scontrol token -u slurm)\n",
    "\n",
    "aws secretsmanager describe-secret --secret-id slurm_token --region $REGION\n",
    "\n",
    "if [ $? -eq 0 ]\n",
    "then\n",
    " aws secretsmanager update-secret --secret-id slurm_token --secret-string \"$SLURM_JWT\" --region $REGION\n",
    "else\n",
    " aws secretsmanager create-secret --name slurm_token --secret-string \"$SLURM_JWT\" --region $REGION\n",
    "fi\n",
    "```\n",
    "\n",
    "#### Step 3. Add a file \"slurm-token\" in /etc/cron.d/\n",
    "\n",
    "```/etc/cron.d/slurm-token\n",
    "# Run the slurm token update every 20 minues \n",
    "SHELL=/bin/bash\n",
    "PATH=/sbin:/bin:/usr/sbin:/usr/bin\n",
    "MAILTO=root\n",
    "*/20 * * * * root /shared/token_refresher.sh                                       \n",
    "```\n",
    "\n",
    "#### Step 4. Add permission to access SecretManager for this notebook\n",
    "\n",
    "Don't forget to add secretsmanager:GetSecretValue permission to the sagemaker execution role that runs this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Slurm REST API Schema\n",
    "\n",
    "Note: The post install script does several things, which will take a about 20 minutes. if you get a ResourceNotFoundException, please wait till the init process complete and run the following block again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "slurm_openapi_ep = 'http://'+slurm_host+':8082/openapi/v3'\n",
    "slurm_rest_base='http://'+slurm_host+':8082/slurm/v0.0.35'\n",
    "\n",
    "try: \n",
    "    _, get_headers = pcluster_helper.update_header_token()\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "        print(\"Token has not been added to Secrets Manager, please wait and try again\")\n",
    "        raise e\n",
    "#else:\n",
    "#    print(get_headers)\n",
    "\n",
    "try:\n",
    "    resp_api = requests.get(slurm_openapi_ep, headers=get_headers)\n",
    "except requests.exceptions.ConnectionError:\n",
    "    resp_api.status_code = \"Connection refused\"    \n",
    "\n",
    "#if resp_api.status_code != 200:\n",
    "#    # This means something went wrong.\n",
    "#    print(\"Error\" , resp.status_code)\n",
    "#    time.sleep(5)\n",
    "\n",
    "#with open('build/slurm_api.json', 'w') as outfile:\n",
    "#    json.dump(resp_api.json(), outfile)\n",
    "\n",
    "print(json.dumps(resp_api.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use REST API callls to interact with ParallelCluster\n",
    "\n",
    "Then we will make direct REST API requests to retrieve the partitions in response\n",
    "\n",
    "If you get server errors, most likely\n",
    "1. Cron job - token_refresher.sh (every 20 mins) hasn't been run yet after the IAM policy is updated. You can check for the slurm_token_yourClusterName secrete in AWS Secret Manager console. \n",
    "2. login to the head-node and check the system logs of \"slurmrestd\", which is running as a service. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used during developemnt, to reload the module after a change in the module\n",
    "try:\n",
    "    del sys.modules['pcluster_athena']\n",
    "except:\n",
    "    #ignore if the module is not loaded\n",
    "    print('Module not loaded, ignore')\n",
    "    \n",
    "from pcluster_athena import PClusterHelper\n",
    "# create the cluster - # You can rerun the rest of the notebook again with no harm. There are checks in place for existing resoources. \n",
    "pcluster_helper = PClusterHelper(pcluster_name)\n",
    "\n",
    "partition_info = [\"name\", \"nodes\", \"nodes_online\", \"total_cpus\", \"total_nodes\"]\n",
    "\n",
    "##### call REST API directly\n",
    "slurm_partitions_url= slurm_rest_base+'/partitions/'\n",
    "partitions = pcluster_helper.get_response_as_json(slurm_partitions_url)\n",
    "\n",
    "#print(partitions['partitions'])\n",
    "#20.02.4 returns a dict, not an array\n",
    "pcluster_helper.print_table_from_dict(partition_info, partitions['partitions'])\n",
    "\n",
    "# newer slurmrest return proper array\n",
    "# print_table_from_json_array(partition_info, [partitions['partitions']['q1'], partitions['partitions']['q2']] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a job\n",
    "The slurm_rest_api_client job submit function doesn't include the \"script\" parameter. We will have to use the REST API Post directly. \n",
    "\n",
    "The body of the post should be like this.  \n",
    "\n",
    "```\n",
    "{\"job\": {\"account\": \"test\", \"ntasks\": 20, \"name\": \"test18.1\", \"nodes\": [2, 4],\n",
    "\"current_working_directory\": \"/tmp/\", \"environment\": {\"PATH\": \"/bin:/usr/bin/:/usr/local/bin/\",\"LD_LIBRARY_PATH\":\n",
    "\"/lib/:/lib64/:/usr/local/lib\"} }, \"script\": \"#!/bin/bash\\necho it works\"}\n",
    "```\n",
    "When the job is submitted through REST API, it will run as the user \"slurm\". That's what the work directory \"/shared/tmp\" should be owned by \"slurm:slurm\", which is done in the post_install script. \n",
    "\n",
    "fetch_and_run.sh will fetch the sbatch script and the input file from S3 and put them in /shared/tmp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program batch script, input and output files\n",
    "\n",
    "To share the pcluster among different users and make sure users can only access their own input and output files, we will use user's ow S3 buckets for input and output files.\n",
    "\n",
    "The job will be running on the ParallelCluster under /efs/tmp (for example) through a fatch (from the S3 bucket) and run script and the output will be stored in the same bucket under \"output\" path. \n",
    "\n",
    "If the simulation results are stored in vtk files, which can be merged into single block vtk files from individual mesh block vtk files. The merging process is programmed in the batch script after the simulation executions. \n",
    "\n",
    "In this notebook, we will use hdf5 format for the output data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Where the batch script, input file, output files are uploaded to S3\n",
    "job_name = \"orszag-tang-mediumres-q1\"\n",
    "my_prefix = \"athena/\"+job_name\n",
    "# fake account_name \n",
    "account_name = \"test-account-1\" \n",
    "partition = \"q1\"\n",
    "use_efa=\"YES\"\n",
    "output_format=\"hdf\" # or \"vtk\"\n",
    "\n",
    "# template files for input and batch script\n",
    "input_file_ini = \"config/athinput_orszag_tang.ini\"\n",
    "batch_file_ini = \"config/batch_athena_sh.ini\"\n",
    "\n",
    "# actual input and batch script files\n",
    "input_file = \"athinput_orszag_tang.input\"\n",
    "batch_file = \"batch_athena.sh\"\n",
    "    \n",
    "###\n",
    "# Mesh/Meshblock parameters\n",
    "# nx1,nx2,nx3 - number of zones in x,y,z\n",
    "# mbx1, mbx2, mbx3 - meshblock size \n",
    "# nx1/mbx1 X nx2/mbx2 X nx3/mbx3 = number of meshblocks - this should be the number of cores you are running the simulation on \n",
    "# e.g. mesh 100 X 100 X 100 with meshsize 50 X 50 X 50 will yield 2X2X2 = 8 blocks, run this on a cluster with 8 cores \n",
    "# test configurations: \n",
    "# highres : 512x512x512 on 64x64x64 meshblock needs 512 cores = 16 nodes on q1\n",
    "# mediumres: 256x256x256 on 64x64x64 meshblock needs 64 cores = 2 nodes on q1 or 16 nodes on q2\n",
    "# mediumres: 256x256x128 on 64x64x64 meshblock needs 32 cores = 1 nodes on q1 or 8 nodes on q2\n",
    "# lowres: 128x128x18 on 64x64x64 meshblock needs 8 cores = 1 node on q1 or 2 nodes on q2 \n",
    "\n",
    "#Mesh - actual domain of the problem \n",
    "# 512X512X512 cells with 64x64x64 meshblock - will have 8X8X8 = 512 meshblocks - if running on 32 cores/node\n",
    "# 512/32=16 nodes\n",
    "nx1=256\n",
    "nx2=256\n",
    "nx3=256\n",
    "\n",
    "#Meshblock - each meshblock size - not too big \n",
    "mbnx1=64\n",
    "mbnx2=64\n",
    "mbnx3=64\n",
    "\n",
    "#Make sure the mesh is divisible by meshblock size\n",
    "# e.g. num_blocks = (512/64)*(512/64)*(512/64) = 8 x 8 x 8 = 512\n",
    "num_blocks = (nx1/mbnx1)*(nx2/mbnx2)*(nx3/mbnx3)\n",
    "\n",
    "###\n",
    "# Batch file parameters\n",
    "# num_nodes should be less than or equal to the max number of nodes in your cluster\n",
    "# num_tasks_per_node should be less than or equal to the max number of nodes in your cluster \n",
    "# e.g. 512 meshblocks / 32 core/node * 1 core/meshblock = 16 nodes -  c5n.18xlarge\n",
    "#num_nodes = 2\n",
    "\n",
    "# e.g. 64 meshblocks / 4 core/node * 1 core/meshblock = 4 nodes - c5n.2xlarge\n",
    "num_nodes = 2\n",
    "num_of_threads = 1\n",
    "\n",
    "num_tasks_per_node = num_blocks/num_nodes/num_of_threads\n",
    "cpus_per_task = num_of_threads\n",
    "\n",
    "\n",
    "\n",
    "#This is where the program is installed on the cluster\n",
    "exe_path = \"/shared/athena-public-version/bin/athena\"\n",
    "#This is where the program is going to run on the cluster\n",
    "work_dir = '/shared/tmp/'+job_name\n",
    "ph = { '${nx1}': str(nx1), \n",
    "       '${nx2}': str(nx2),\n",
    "       '${nx3}': str(nx3),\n",
    "       '${mbnx1}': str(mbnx1),\n",
    "       '${mbnx2}': str(mbnx2),\n",
    "       '${mbnx3}': str(mbnx3), \n",
    "       '${num_of_threads}' : str(num_of_threads)}\n",
    "pcluster_helper.template_to_file(input_file_ini, 'build/'+input_file, ph)\n",
    "\n",
    "ph = {'${nodes}': str(num_nodes),\n",
    "      '${ntasks-per-node}': str(int(num_tasks_per_node)),\n",
    "      '${cpus-per-task}': str(cpus_per_task),\n",
    "      '${account}': account_name,\n",
    "      '${partition}': partition,\n",
    "      '${job-name}': job_name,\n",
    "      '${EXE_PATH}': exe_path,\n",
    "      '${WORK_DIR}': work_dir,\n",
    "      '${input-file}': input_file,\n",
    "      '${BUCKET_NAME}': my_bucket_name,\n",
    "      '${PREFIX}': my_prefix,\n",
    "      '${USE_EFA}': use_efa,\n",
    "      '${OUTPUT_FOLDER}': \"output/\",\n",
    "      '${OUTPUT_FORMAT}': output_format,\n",
    "      '${NUM_OF_THREADS}' : str(num_of_threads)}\n",
    "pcluster_helper.template_to_file(batch_file_ini, 'build/'+batch_file, ph)\n",
    "\n",
    "# create batch and \n",
    "def upload_athena_files(input_file, batch_file):\n",
    "    session = boto3.Session()\n",
    "    s3_client = session.client('s3')\n",
    "\n",
    "    try:\n",
    "        resp = s3_client.upload_file('build/'+input_file, my_bucket_name, my_prefix+'/'+input_file)\n",
    "        resp = s3_client.upload_file('build/'+batch_file, my_bucket_name, my_prefix+'/'+batch_file)\n",
    "    except ClientError as e:\n",
    "        print(e)\n",
    "\n",
    "# upload to S3 for use later\n",
    "upload_athena_files(input_file, batch_file)\n",
    "\n",
    "job_script = \"#!/bin/bash\\n/shared/tmp/fetch_and_run.sh {} {} {} {} {}\".format(my_bucket_name, my_prefix, input_file, batch_file, job_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "slurm_job_submit_base=slurm_rest_base+'/job/submit'\n",
    "\n",
    "job_script = \"#!/bin/bash\\n/shared/tmp/fetch_and_run.sh {} {} {} {} {}\".format(my_bucket_name,my_prefix, input_file, batch_file, job_name)\n",
    "\n",
    "#in order to use Slurm REST to submit jobs, you need to have the working directory permission set to nobody:nobody. in this case /efs/tmp\n",
    "data = {'job':{ 'account': account_name, 'partition':partition , 'name': job_name, 'current_working_directory':'/shared/tmp/', 'environment': {\"PATH\": \"/bin:/usr/bin/:/usr/local/bin/:/opt/slurm/bin:/opt/amazon/openmpi/bin\",\"LD_LIBRARY_PATH\":\n",
    "\"/lib/:/lib64/:/usr/local/lib:/opt/slurm/lib:/opt/slurm/lib64\"}}, 'script':job_script}\n",
    "\n",
    "###\n",
    "# This job submission will generate two jobs , the job_id returned in the response is for the bash job itself. the sbatch will be the job_id+1 run subsequently.\n",
    "#\n",
    "resp_job_submit = pcluster_helper.post_response_as_json(slurm_job_submit_base, data=json.dumps(data))\n",
    "\n",
    "\n",
    "print(resp_job_submit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List recent jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all the jobs immediately after the previous step. This should return two running jobs. \n",
    "slurm_jobs_base=slurm_rest_base+'/jobs'\n",
    "\n",
    "jobs = pcluster_helper.get_response_as_json(slurm_jobs_base)\n",
    "# print(jobs)\n",
    "jobs_headers = [ 'job_id', 'job_state', 'account', 'batch_host', 'nodes', 'cluster', 'partition', 'current_working_directory']\n",
    "\n",
    "# newer version of slurm \n",
    "#print_table_from_json_array(jobs_headers, jobs['jobs'])\n",
    "pcluster_helper.print_table_from_json_array(jobs_headers, jobs)\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Athena++ Simulation Results\n",
    "In this notebook, we are going to use the python library comes with Athena++ to read and visualize the simulation results.\n",
    "\n",
    "In the previous notebook, we saved the simulation results in s3://<bucketname>/athema/$job_name/output folder\n",
    "\n",
    "Import the hdf python code that came with Athena++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import clear_output\n",
    "import h5py\n",
    "\n",
    "#Do this once. clone the athena++ source code , and the hdf5 python package we need is under vis/python folder\n",
    "\n",
    "if not os.path.isdir('athena-public-version'):\n",
    "    !git clone https://github.com/PrincetonUniversity/athena-public-version\n",
    "else:\n",
    "    print(\"Athena++ code already cloned, skip\")\n",
    "    \n",
    "sys.path.insert(0, 'athena-public-version/vis/python')\n",
    "import athena_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=job_name+'/output'\n",
    "output_folder = my_bucket_name+'/athena/'+data_folder\n",
    "\n",
    "if not os.path.isdir(job_name):\n",
    "    !mkdir -p $job_name\n",
    "else:\n",
    "    !rm -rf $job_name/*\n",
    "    print('project folder exists, remove all old files')\n",
    "    \n",
    "!aws s3 cp s3://$output_folder/ ./$data_folder/ --recursive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the hst data\n",
    "History data shows the overs all parameter changes over time. The time interval can be different from that of the hdf5 files.\n",
    "\n",
    "In OrszagTang simulations, the variables in the hst files are 'time', 'dt', 'mass', '1-mom', '2-mom', '3-mom', '1-KE', '2-KE', '3-KE', 'tot-E', '1-ME', '2-ME', '3-ME'\n",
    "\n",
    "All the variables a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hst = athena_read.hst(data_folder+'/OrszagTang.hst')\n",
    "\n",
    "# cannot use this reliably because hst and hdf can have different number of time steps. In this case,we have the same number of steps\n",
    "num_timesteps = len(hst['time'])\n",
    "\n",
    "print(hst.keys())\n",
    "\n",
    "plt.plot(hst['time'], hst['dt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading HDF5 data files \n",
    "\n",
    "The hdf5 data files contain all variables inside all meshblocks. There are some merging and calculating work to be done before we can visualizing the result. Fortunately ,Athena++ vis/hdf package takes care of the hard part. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's example the content of the hdf files\n",
    "\n",
    "f = h5py.File(data_folder+'/OrszagTang.out2.00001.athdf', 'r')\n",
    "# variable lists <KeysViewHDF5 ['B', 'Levels', 'LogicalLocations', 'prim', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v']>\n",
    "print(f.keys())\n",
    "\n",
    "#<HDF5 dataset \"B\": shape (3, 512, 64, 64, 64), type \"<f4\"> \n",
    "print(f['prim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation result data \n",
    "\n",
    "Raw athdf data has the following keys\n",
    "<KeysViewHDF5 ['B', 'Levels', 'LogicalLocations', 'prim', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v']>\n",
    "\n",
    "After athena_read.athdf() call, the result contains keys, which can be used as the field name\n",
    "['Coordinates', 'DatasetNames', 'MaxLevel', 'MeshBlockSize', 'NumCycles', 'NumMeshBlocks', 'NumVariables', 'RootGridSize', 'RootGridX1', 'RootGridX2', 'RootGridX3', 'Time', 'VariableNames', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v', 'rho', 'press', 'vel1', 'vel2', 'vel3', 'Bcc1', 'Bcc2', 'Bcc3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_athdf(filename, num_step):\n",
    "    print(\"Processing \", filename)\n",
    "    athdf = athena_read.athdf(filename)\n",
    "    return athdf\n",
    "\n",
    "# extract list of fields and take a slice in one dimension, dimension can be 'x', 'y', 'z'\n",
    "def read_all_timestep (data_file_name_template, num_steps, field_names, slice_number, dimension):\n",
    "\n",
    "    if not dimension in ['x', 'y', 'z']:\n",
    "        print(\"dimension can only be 'x/y/z'\")\n",
    "        return\n",
    "    \n",
    "    # would ideally process all time steps together and store themn in memory. However, they are too big, will have to trade time for memory \n",
    "    result = {}\n",
    "    for f in field_names:\n",
    "        result[f] = list()\n",
    "        \n",
    "    for i in range(num_steps):\n",
    "        fn = data_file_name_template.format(str(i).zfill(5))\n",
    "        athdf = process_athdf(fn, i)\n",
    "        for f in field_names:\n",
    "            if dimension == 'x':\n",
    "                result[f].append(athdf[f][slice_number,:,:])\n",
    "            elif dimension == 'y':\n",
    "                result[f].append(athdf[f][:, slice_number,:])\n",
    "            else:\n",
    "                result[f].append(athdf[f][:,:, slice_number])\n",
    "                        \n",
    "    return result\n",
    "\n",
    "def animate_slice(data):\n",
    "    plt.figure()\n",
    "    for i in range(len(data)):\n",
    "        plt.imshow(data[i])\n",
    "        plt.title('Frame %d' % i)\n",
    "        plt.show()\n",
    "        plt.pause(0.2)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_file_name_template = data_folder+'/OrszagTang.out2.{}.athdf'\n",
    "\n",
    "# this is time consuming, try do it once\n",
    "data = read_all_timestep(data_file_name_template, num_timesteps, ['press', 'rho'], 1, 'x')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle through the time steps and look at pressure\n",
    "animate_slice(data['press'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at density\n",
    "animate_slice(data['rho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't forget to clean up\n",
    "\n",
    "1. Delete the ParallelCluster\n",
    "2. Delete the RDS\n",
    "3. S3 bucket\n",
    "4. Secrets used in this excercise\n",
    "\n",
    "Deleting VPC is risky, I will leave it out for you to manually clean it up if you created a new VPC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to do this first. \n",
    "iam_client.detach_role_policy(RoleName=root_role_name, PolicyArn='arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore')\n",
    "\n",
    "!pcluster delete $pcluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the rds database - keep it if you want to have more records to look at\n",
    "#workshop.detele_rds_instance(region, session, db_name)\n",
    "#workshop.delete_secrets_with_force(region, session, [rds_secret_name])\n",
    "\n",
    "#Delete the secrets\n",
    "workshop.delete_secrets_with_force(region, session, [slurm_secret_name])\n",
    "\n",
    "workshop.delete_bucket_completely(my_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
