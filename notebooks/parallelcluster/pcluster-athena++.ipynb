{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AWS ParallelCluster\n",
    "\n",
    "## Create a cluster\n",
    "If you have not installed aws-parallelcluster commandline tool, uncomment the next line of code and executed it. You only need to do it once. \n",
    "\n",
    "If you have installed \"pcluster\" command correctly, it should return \"2.10.0\"\n",
    "\n",
    "Before you start, please have the following pre-requisites ready. \n",
    "* A VPC that has a public subnet with an internet gateway\n",
    "* A MySQL RDS database in the same subnet\n",
    "* The SageMaker execution role used for this notebook have permission to create a ParallelCluster, create EC2 keypair and VPCs \n",
    "\n",
    "Details about the policies are described in this document. \n",
    "https://docs.aws.amazon.com/parallelcluster/latest/ug/iam.html#parallelclusteruserpolicy\n",
    "\n",
    "As an alternative, you can create a IAM user that has the policies mentioned above, and add the aws_access_key_id and aws_secret_access_key in the [aws] section of the following config file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import base64\n",
    "import docker\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import project_path # path to helper methods\n",
    "from lib import workshop\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "ec2_client = boto3.client('ec2')\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "# Get the aws account number where this notebook, the cluster sits. It's used in IAM policy resource\n",
    "my_account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# specify the following names\n",
    "\n",
    "# ssh key for access the pcluster. this key is not needed  in this excercise, but useful if you need to ssh into the headnode of the pcluster\n",
    "key_name = 'pcluster-athena-key'\n",
    "keypair_saved_path = './'+key_name+'.pem'\n",
    "# unique name of the pcluster\n",
    "pcluster_name = 'myTestCluster'\n",
    "# the rds for the Slurmdbd datastore. We will use a MySQL server as the data store. Server's hostname, username, password will be saved in a secret in Secrets Manager\n",
    "rds_secret_name = 'slurm_dbd_credential'\n",
    "# the slurm REST token is generated from the headnode and stored in Secrets Manager. This token is used in makeing REST API calls to the Slurm REST endpoint running on the headnode \n",
    "slurm_secret_name = \"slurm_token_{}\".format(pcluster_name)\n",
    "# We only need one subnet for the pcluster, but two subnets are needed for RDS instance. If use existing VPC, we will use the default VPC, and the first subnet in default VPC\n",
    "use_existing_vpc = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During development, everytime you update the workshop module, you need to call this:  \n",
    "importlib.reload(workshop)\n",
    "\n",
    "# we will not need to use the ssh_key in this excercise. However, you can only download the key once during creation. we will save it in case\n",
    "try:\n",
    "    workshop.create_keypair(region, session, key_name, keypair_saved_path)\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"InvalidKeyPair.Duplicate\":\n",
    "        print(\"KeyPair with the name {} alread exists. Skip\".format(key_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VPC\n",
    "\n",
    "You can use the existing default VPC or create a new VPC with 2 subnets. \n",
    "\n",
    "We will only be using one of the subnets for the ParallelCluster, but both are used for the RDS database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if use_existing_vpc:\n",
    "    vpc_filter = [{'Name':'isDefault', 'Values':['true']}]\n",
    "    default_vpc = ec2_client.describe_vpcs(Filters=vpc_filter)\n",
    "    vpc_id = default_vpc['Vpcs'][0]['VpcId']\n",
    "\n",
    "    subnet_filter = [{'Name':'vpc-id', 'Values':[vpc_id]}]\n",
    "    subnets = ec2_client.describe_subnets(Filters=subnet_filter)\n",
    "    subnet_id = subnets['Subnets'][0]['SubnetId']\n",
    "    subnet_id2 = subnets['Subnets'][1]['SubnetId']    \n",
    "else: \n",
    "    vpc, subnet1, subnet2 = workshop.create_and_configure_vpc()\n",
    "    vpc_id = vpc.id\n",
    "    subnet_id = subnet1.id\n",
    "    subnet_id2 = subnet2.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the project bucket. \n",
    "# we will use this bucket for the scripts, input and output files \n",
    "\n",
    "\n",
    "bucket_prefix = pcluster_name.lower()+'-'+my_account_id\n",
    "\n",
    "# use the bucket prefix as name, don't use uuid suffix\n",
    "my_bucket_name = workshop.create_bucket(region, session, bucket_prefix, False)\n",
    "print(my_bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SPOT Instances\n",
    "We will create two queues in this excercise, one using on-demand instances and one using SPOT instances. To use SPOT, we need AWSServiceRoleForEC2SpotFleet service-linked role in this account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    iam_client.get_role(RoleName=\"AWSServiceRoleForEC2SpotFleet\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "        print(\"AWSServiceRoleForEC2SpotFleet doesn't exist, create one ... \")\n",
    "        iam_client.create_service_linked_role(AWSServiceName='spotfleet.amazonaws.com')\n",
    "        print(\"AWSServiceRoleForEC2SpotFleet created successfully\")\n",
    "else: \n",
    "    print(\"AWSServiceRoleForEC2SpotFleet exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDS Database (MySQL) - used with ParallelCluster for accounting\n",
    "\n",
    "We will create a simple MySQL RDS database instance to use as a data store for Slurmdbd for accounting. The username and password are stored as a secret in the Secrets Manager. \n",
    "The secret is later used to configure Slurmdbd. \n",
    "\n",
    "The RDS instance will be created asynchronuously. While the secret is created immediated, the hostname will be available only after the creation is completed. We will have to update the hostname in the secreat afterwards. \n",
    "\n",
    "We will update the security group to allow traffic to port 3306 from the cluster in the same vpc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RDS for cluster accounting\n",
    "importlib.reload(workshop)\n",
    "\n",
    "db_name = 'pclusterdb'\n",
    "\n",
    "# create a simple mysql rds instance , the username and password will be stored in secrets maanger as a secret\n",
    "workshop.create_simple_mysql_rds(region, session, db_name, [subnet_id,subnet_id2] ,rds_secret_name)\n",
    "\n",
    "\n",
    "rds_client = session.client('rds', region)\n",
    "rds_waiter = rds_client.get_waiter('db_instance_available')\n",
    "\n",
    "print(\"Waiting for the DB creation to finish ... \")\n",
    "try:\n",
    "    rds_waiter.wait(DBInstanceIdentifier=db_name) \n",
    "except botocore.exceptions.WaiterError as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"Finished creating the db.\")\n",
    "\n",
    "#since the rds creation is asynch, need to wait till the creation is done to get the hostname, then update the secret with the hostname\n",
    "vpc_sgs = workshop.get_sgs_and_update_secret(region, session, db_name, rds_secret_name)\n",
    "print(vpc_sgs)\n",
    "\n",
    "# Step 3. get the vpc local CIDR range \n",
    "ec2 = boto3.resource('ec2')\n",
    "vpc = ec2.Vpc(vpc_id)\n",
    "cidr = vpc.cidr_block\n",
    "\n",
    "# update the RDS security group to allow inbound traffic to port 3306\n",
    "workshop.update_security_group(vpc_sgs[0]['VpcSecurityGroupId'], cidr, 3306)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vpc_sgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pcluster CLI\n",
    "You only need to do this once in this kernel. If you have not installed pcluster, uncomment the next two lines and run the block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install --upgrade pip\n",
    "#!sudo pip3 install --upgrade aws-parallelcluster\n",
    "!pcluster version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParallelCluster config file\n",
    "Start with the the configuration template file \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat config/config.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup parameters for PCluster\n",
    "\n",
    "We will be using a relational database on AWS (RDS) for Slurm accounting (slurmdbd). Please refer to this blog for how to set it up https://aws.amazon.com/blogs/compute/enabling-job-accounting-for-hpc-with-aws-parallelcluster-and-amazon-rds/\n",
    "\n",
    "Once you set up the MySQL RDS, create a secret in SecretManager with the type \"Credentials for RDS\", so we don't need to expose the database username/password in plain text in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### assuem you have created a database secret in SecretManager with the name \"slurm_dbd_credential\"\n",
    "def get_slurm_dbd_rds_secret():\n",
    "    secret_name = rds_secret_name\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "    \n",
    "###\n",
    "# helper function to replace all place_holders in a string\n",
    "# content is a string, values is a dict with placeholder name and value as attributes\n",
    "###\n",
    "def replace_placeholder(content, values):\n",
    "    for k,v in values.items():\n",
    "        content=content.replace(k, v)\n",
    "    return content\n",
    "\n",
    "def template_to_file(source_file, target_file, mapping):\n",
    "    with open(source_file, \"rt\") as f:\n",
    "        content = f.read()\n",
    "        with open(target_file, \"wt\") as fo:\n",
    "            fo.write(replace_placeholder(content, mapping))\n",
    "    \n",
    "    \n",
    "    \n",
    "# the response is a json {\"username\": \"xxxx\", \"password\": \"xxxx\", \"engine\": \"mysql\", \"host\": \"xxxx\", \"port\": \"xxxx\", \"dbInstanceIdentifier\", \"xxxx\"}\n",
    "rds_secret = json.loads(get_slurm_dbd_rds_secret())\n",
    "\n",
    "post_install_script_prefix = 'scripts/post_install_script.sh'\n",
    "post_install_script_location = \"s3://{}/{}\".format(my_bucket_name, post_install_script_prefix)\n",
    "post_install_script_args = \"'\" + rds_secret['host']+' '+str(rds_secret['port']) +' ' + rds_secret['username'] + ' ' + rds_secret['password'] + ' ' + pcluster_name +\"'\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post installation script\n",
    "This script is used to recompile and configure slurm with slurmrestd. We also added the automation of compiling Athena++ in the script. \n",
    "\n",
    "Let's take a look at the scrupt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat scripts/pcluster_post_install.sh\n",
    "\n",
    "#upload the script to S3\n",
    "session = boto3.Session()\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "try:\n",
    "    resp = s3_client.upload_file('scripts/pcluster_post_install.sh', my_bucket_name, post_install_script_prefix)\n",
    "except ClientError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the placeholder with value in config.ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = {'${REGION}': region, \n",
    "      '${VPC_ID}': vpc_id, \n",
    "      '${SUBNET_ID}': subnet_id, \n",
    "      '${KEY_NAME}': key_name, \n",
    "      '${POST_INSTALL_SCRIPT_LOCATION}': post_install_script_location, \n",
    "      '${POST_INSTALL_SCRIPT_ARGS}': post_install_script_args\n",
    "     }\n",
    "\n",
    "template_to_file(\"config/config.ini\", \"build/config\", ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat build/config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pcluster with the config file\n",
    "\n",
    "The -nr note is used to tell cloudformation not to roll back when there is an error - this is only needed for development. \n",
    "\n",
    "After the cluster is created, we will use boto to setup the following permissions\n",
    "1. Add IAM permission on the head-node instance role to allow access to Secret Manager for storing slurm token \n",
    "2. Add Inbound rule to allow \"All traffic\" from the SageMaker notebook instance (for Slurmrest API access)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pcluster create $pcluster_name -nr -c build/config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update IAM policy and security group \n",
    "\n",
    "Use boto3 to \n",
    "1. Update a policy in parallelcluster head-node instance role, to allow the head-node to access Secret Manager.\n",
    "2. Add inbound rule to allow access to the REST API from this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the lib during development\n",
    "importlib.reload(workshop)\n",
    "# Use the stack name to find the resources created with the parallelcluster. Use some of the information to update\n",
    "# the IAM policy and security group\n",
    "cluster_stack_name = 'parallelcluster-'+pcluster_name\n",
    "\n",
    "\n",
    "#Step 1. Get the head-node's instanace role and headnode security group \n",
    "cf_client = boto3.client('cloudformation')\n",
    "root_role_info = cf_client.describe_stack_resource(StackName=cluster_stack_name, LogicalResourceId='RootRole' )\n",
    "sg_info = cf_client.describe_stack_resource(StackName=cluster_stack_name, LogicalResourceId='MasterSecurityGroup' )\n",
    "\n",
    "#Root role  and security group physical resource id\n",
    "root_role_name = root_role_info['StackResourceDetail']['PhysicalResourceId']\n",
    "head_sg_name = sg_info['StackResourceDetail']['PhysicalResourceId']\n",
    "\n",
    "# Step 2. Add Secret Manager access permission to the role\n",
    "# pcluster will create an inline policy \"parallelcluster\" and attach to the root role, we will update that\n",
    "\n",
    "policy_doc = iam_client.get_role_policy(RoleName=root_role_name, PolicyName=\"parallelcluster\").get('PolicyDocument')\n",
    "policy_statement = policy_doc.get('Statement')    \n",
    "\n",
    "# in this notebook, we might re-run this block multiple times , to avoid duplication of the tokensecret sid need to do this loop\n",
    "flag = False\n",
    "sid = 'tokensecret'\n",
    "for stmt in policy_statement:\n",
    "    if ( stmt ['Sid'] == sid):\n",
    "        flag = True\n",
    "        print(\"{} statement is already there, skip\".format(sid))\n",
    "        break\n",
    "\n",
    "if not flag :        \n",
    "    my_doc_statement = {}\n",
    "    my_doc_statement['Action'] = ['secretsmanager:DescribeSecret','secretsmanager:CreateSecret','secretsmanager:UpdateSecret']\n",
    "    my_doc_statement['Resource'] = ['arn:aws:secretsmanager:us-east-1:{}:secret:*'.format(my_account_id)]\n",
    "    my_doc_statement['Effect'] = 'Allow'\n",
    "    my_doc_statement['Sid'] = sid\n",
    "    policy_doc['Statement'].append(my_doc_statement)\n",
    "    iam_client.put_role_policy(RoleName=root_role_name,PolicyName=\"parallelcluster\", PolicyDocument=json.dumps(policy_doc))\n",
    "\n",
    "# Step 3. get the vpc local CIDR range \n",
    "ec2 = boto3.resource('ec2')\n",
    "vpc = ec2.Vpc(vpc_id)\n",
    "cidr = vpc.cidr_block\n",
    "\n",
    "workshop.update_security_group(head_sg_name, cidr, 8082)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the pcluster head-node PrivateIP \n",
    "# ! cmd returns a IPython.utils.text.SList, which has grep, fields methods and s,n,p properties\n",
    "####\n",
    "pcluster_status = !pcluster status $pcluster_name\n",
    "\n",
    "# get the second part of 'MasterPrivateIP: 172.16.2.92'\n",
    "slurm_host = pcluster_status.grep('MasterPrivateIP').s.split()[1]\n",
    "\n",
    "print(slurm_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate with Slurm REST API running on the head node\n",
    "\n",
    "SLURM REST is currently running on the headnode, using jwt as the auth mechanism. Follow this WIKI to set up the Slurm REST API endpoint on the head-node: https://w.amazon.com/bin/view/Users/jianjx/hpc/howto-slurmrestd/\n",
    "\n",
    "In the post_install script, slurmrestd is enabled to run as a daemon with the following command on the head-node. \n",
    " \n",
    "```\n",
    "sudo systemctl start slurmrestd\n",
    "```\n",
    "\n",
    "First from the openapi endpoint /openapi/v3, dump the API json to a file, which in turn can be used to generate a client API package using openapi-python-client (https://pypi.org/project/openapi-python-client/)  - Note: this client package doesn't quite work as expected. We will be using direct REST API calls. But I will leave some code here to show you how it works.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Store the JWT token in Secrete Manager\n",
    "JWT token can be created using the \"scontrol token username=slurm\" command on the head-node. To pass it securely to this notebook, we will first create a cron job on the headnode to retrieve the token, then save it in SecreteManager with a name \"slurm_token\". The default JWT token lifespan is 1800 seconds(30 mins). Run the follow script on the head-node as a cron job to update the token every 20 mins\n",
    "\n",
    "\n",
    "#### Step 1.  Add permission to the instance role for the head-node\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Action\": [ \n",
    "        \"secretsmanager:DescribeSecret\",\n",
    "        \"secretsmanager:CreateSecret\",\n",
    "        \"secretsmanager:UpdateSecret\"],\n",
    "    \"Resource\": [\n",
    "        \"arn:aws:secretsmanager:us-east-1:<account-id>:secret:*\"\n",
    "    ],\n",
    "    \"Effect\": \"Allow\",\n",
    "    \"Sid\": \"tokensecret\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Skip step 2 and step 3 - They are now included in the post_install script**\n",
    "\n",
    "#### Step 2. Create a script \"token_refresher.sh\" \n",
    "Assume we save the following script at /shared/token_refresher.sh \n",
    "\n",
    "``` token_refresher.sh\n",
    "#!/bin/bash\n",
    "\n",
    "REGION=us-east-1\n",
    "export $(/opt/slurm/bin/scontrol token -u slurm)\n",
    "\n",
    "aws secretsmanager describe-secret --secret-id slurm_token --region $REGION\n",
    "\n",
    "if [ $? -eq 0 ]\n",
    "then\n",
    " aws secretsmanager update-secret --secret-id slurm_token --secret-string \"$SLURM_JWT\" --region $REGION\n",
    "else\n",
    " aws secretsmanager create-secret --name slurm_token --secret-string \"$SLURM_JWT\" --region $REGION\n",
    "fi\n",
    "```\n",
    "\n",
    "#### Step 3. Add a file \"slurm-token\" in /etc/cron.d/\n",
    "\n",
    "```/etc/cron.d/slurm-token\n",
    "# Run the slurm token update every 20 minues \n",
    "SHELL=/bin/bash\n",
    "PATH=/sbin:/bin:/usr/sbin:/usr/bin\n",
    "MAILTO=root\n",
    "*/20 * * * * root /shared/token_refresher.sh                                       \n",
    "```\n",
    "\n",
    "#### Step 4. Add permission to access SecretManager for this notebook\n",
    "\n",
    "Don't forget to add secretsmanager:GetSecretValue permission to the sagemaker execution role that runs this notebook\n",
    "\n",
    "TODO: use boto3 to add permission for the rootRole in the cluster to have access to the Secret Manager\n",
    "enable security group on :8082 from VPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import base64\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "slurm_user = 'slurm'\n",
    "\n",
    "def display_table(data):\n",
    "    html = \"<table>\"\n",
    "    for row in data:\n",
    "        html += \"<tr>\"\n",
    "        for field in row:\n",
    "            html += \"<td><h4>%s</h4><td>\"%(field)\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "###\n",
    "# Retrieve the slurm_token from the SecretManager\n",
    "#\n",
    "def get_secret():\n",
    "    secret_name = \"slurm_token_{}\".format(pcluster_name)\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        print(\"Error\", e)\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "###\n",
    "# Retrieve the token and inject into the header for JWT auth\n",
    "#\n",
    "def update_header_token():\n",
    "    token = get_secret()\n",
    "    post_headers = {'X-SLURM-USER-NAME':slurm_user, 'X-SLURM-USER-TOKEN': token, 'Content-type': 'application/json', 'Accept': 'application/json'}\n",
    "    get_headers = {'X-SLURM-USER-NAME':slurm_user, 'X-SLURM-USER-TOKEN': token, 'Content-type': 'application/x-www-form-urlencoded', 'Accept': 'application/json'}\n",
    "    return [post_headers, get_headers]\n",
    "\n",
    "###\n",
    "# Convert response into json\n",
    "#\n",
    "def convert_response(resp):\n",
    "    resp_str = resp.content.decode('utf-8')\n",
    "    return json.loads(resp_str)\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "# Print a json array in table format\n",
    "# input: headers [json attribute name, ... ]\n",
    "# input: a - array of json objects\n",
    "def print_table_from_json_array(headers, a):\n",
    "    # add headers as the first row.\n",
    "    t = [headers]\n",
    "    for item in a:\n",
    "        result = []\n",
    "        for h in headers:\n",
    "            result.append(item[h])\n",
    "        t.append(result)\n",
    "    display_table(t)\n",
    "\n",
    "def print_table_from_dict(headers, d):\n",
    "    result = list()\n",
    "    for k,v in d.items():\n",
    "        result.append(v)\n",
    "    print_table_from_json_array(headers, result)\n",
    "        \n",
    "\n",
    "### \n",
    "# wrapper for get\n",
    "#\n",
    "def get_response_as_json(base_url):\n",
    "    _, get_headers = update_header_token()\n",
    "    resp = requests.get(base_url, headers=get_headers)\n",
    "    if resp.status_code != 200:\n",
    "        # This means something went wrong.\n",
    "        print(\"Error\" , resp.status_code)\n",
    "\n",
    "    return convert_response(resp)\n",
    "\n",
    "### \n",
    "# wrapper for post\n",
    "#\n",
    "def post_response_as_json(base_url, data):\n",
    "    post_headers, _ = update_header_token()\n",
    "    resp = requests.post(base_url, headers=post_headers, data=data)\n",
    "    if resp.status_code != 200:\n",
    "        # This means something went wrong.\n",
    "        print(\"Error\" , resp.status_code)\n",
    "\n",
    "    return convert_response(resp)\n",
    "\n",
    "###\n",
    "# Epoch time conversion\n",
    "#\n",
    "def get_localtime(t):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Slurm REST API Schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "slurm_openapi_ep = 'http://'+slurm_host+':8082/openapi/v3'\n",
    "slurm_rest_base='http://'+slurm_host+':8082/slurm/v0.0.35'\n",
    "\n",
    "try: \n",
    "    _, get_headers = update_header_token()\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "        print(\"Token has not been added to Secrets Manager, please wait and try again\")\n",
    "        raise e\n",
    "\n",
    "resp_api = requests.get(slurm_openapi_ep, headers=get_headers)\n",
    "print(resp_api)\n",
    "    \n",
    "if resp_api.status_code != 200:\n",
    "    # This means something went wrong.\n",
    "    print(\"Error\" , resp.status_code)\n",
    "    time.sleep(5)\n",
    "\n",
    "with open('build/slurm_api.json', 'w') as outfile:\n",
    "    json.dump(resp_api.json(), outfile)\n",
    "\n",
    "print(json.dumps(resp_api.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use REST API callls to interact with ParallelCluster\n",
    "\n",
    "Then we will make direct REST API requests to retrieve the partitions in response\n",
    "\n",
    "If you get server errors, most likely\n",
    "1. Cron job - token_refresher.sh (every 20 mins) hasn't been run yet after the IAM policy is updated. You can check for the slurm_token_yourClusterName secrete in AWS Secret Manager console. \n",
    "2. login to the head-node and check the system logs of \"slurmrestd\", which is running as a service. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partition_info = [\"name\", \"nodes\", \"nodes_online\", \"total_cpus\", \"total_nodes\"]\n",
    "\n",
    "##### This works as well, \n",
    "# update header in case the token has expired\n",
    "_, get_headers = update_header_token()\n",
    "\n",
    "##### call REST API directly\n",
    "slurm_partitions_url= slurm_rest_base+'/partitions/'\n",
    "partitions = get_response_as_json(slurm_partitions_url)\n",
    "#print(partitions['partitions'])\n",
    "#20.02.4 returns a dict, not an array\n",
    "print_table_from_dict(partition_info, partitions['partitions'])\n",
    "\n",
    "# newer slurmrest return proper array\n",
    "# print_table_from_json_array(partition_info, [partitions['partitions']['q1'], partitions['partitions']['q2']] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a job\n",
    "The slurm_rest_api_client job submit function doesn't include the \"script\" parameter. We will have to use the REST API Post directly. \n",
    "\n",
    "The body of the post should be like this.  \n",
    "\n",
    "```\n",
    "{\"job\": {\"account\": \"test\", \"ntasks\": 20, \"name\": \"test18.1\", \"nodes\": [2, 4],\n",
    "\"current_working_directory\": \"/tmp/\", \"environment\": {\"PATH\": \"/bin:/usr/bin/:/usr/local/bin/\",\"LD_LIBRARY_PATH\":\n",
    "\"/lib/:/lib64/:/usr/local/lib\"} }, \"script\": \"#!/bin/bash\\necho it works\"}\n",
    "```\n",
    "When the job is submitted through REST API, it will run as the user \"slurm\". That's what the work directory \"/shared/tmp\" should be owned by \"slurm:slurm\", which is done in the post_install script. \n",
    "\n",
    "fetch_and_run.sh will fetch the sbatch script and the input file from S3 and put them in /shared/tmp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program batch script, input and output files\n",
    "\n",
    "To share the pcluster among different users and make sure users can only access their own input and output files, we will use user's ow S3 buckets for input and output files.\n",
    "\n",
    "The job will be running on the ParallelCluster under /efs/tmp (for example) through a fatch (from the S3 bucket) and run script and the output will be stored in the same bucket under \"output\" path. \n",
    "\n",
    "If the simulation results are stored in vtk files, which are merged into single block vtk files from individual mesh block vtk files. The merging process is programmed in the batch script after the simulation executions. \n",
    "\n",
    "```\n",
    "list=$(ls | grep block |grep vtk)\n",
    "\n",
    "MAXBLOCK_ID=0\n",
    "MAX_OUTSTEP=0\n",
    "for l in $list\n",
    "do\n",
    "  PROB_ID=$(echo $l |cut -d '.' -f 1)\n",
    "  OUTPUT_ID=$(echo $l| cut -d '.' -f 3 |cut --complement -c 1-3)\n",
    "\n",
    "  mb_id=$(echo $l| cut -d '.' -f 2 | cut --complement -c 1-5)\n",
    "  MAXBLOCK_ID=$(( mb_id > MAXBLOCK_ID? mb_id: MAXBLOCK_ID))\n",
    "\n",
    "  os_id=$(echo $l| cut -d '.' -f 4)\n",
    "  os_id=$((10#$os_id))\n",
    "  MAX_OUTSTEP=$(( os_id > MAX_OUTSTEP? os_id: MAX_OUTSTEP))\n",
    "done\n",
    "\n",
    "cp /shared/athena-public-version/vis/vtk/join_* .\n",
    "gcc -o join_vtk++ join_vtk++.c\n",
    "./join_all_vtk.sh $PROB_ID $OUTPUT_ID $MAXBLOCK_ID $MAX_OUTSTEP\n",
    "\n",
    "# copy the output files to S3 , excluding the block files\n",
    "aws s3 cp . s3://$BUCKET_NAME/$PREFIX/$OUTPUT_FOLDER/ --recursive --exclude \"*.block*\"\n",
    "```\n",
    "\n",
    "In this notebook, we will use hdf5 format for the output data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Where the batch script, input file, output files are uploaded to S3\n",
    "job_name = \"orszag-tang-lowres\"\n",
    "my_prefix = \"athena/\"+job_name\n",
    "\n",
    "# template files for input and batch script\n",
    "input_file_ini = \"config/athinput_orszag_tang.ini\"\n",
    "batch_file_ini = \"config/batch_athena_sh.ini\"\n",
    "\n",
    "# actual input and batch script files\n",
    "input_file = \"athinput_orszag_tang.input\"\n",
    "batch_file = \"batch_athena.sh\"\n",
    "    \n",
    "###\n",
    "# Mesh/Meshblock parameters\n",
    "# nx1,nx2,nx3 - number of zones in x,y,z\n",
    "# mbx1, mbx2, mbx3 - meshblock size \n",
    "# nx1/mbx1 X nx2/mbx2 X nx3/mbx3 = number of meshblocks - this should be the number of cores you are running the simulation on \n",
    "# e.g. mesh 100 X 100 X 100 with meshsize 50 X 50 X 50 will yield 2X2X2 = 8 blocks, run this on a cluster with 8 cores \n",
    "# \n",
    "\n",
    "#Mesh - actual domain of the problem \n",
    "# 512X512X512 cells with 64x64x64 meshblock - will have 8X8X8 = 512 meshblocks - if running on 32 cores/node, will need \n",
    "# 512/32=16 nodes\n",
    "nx1=128\n",
    "nx2=128\n",
    "nx3=128\n",
    "\n",
    "#Meshblock - each meshblock size - not too big \n",
    "mbnx1=64\n",
    "mbnx2=64\n",
    "mbnx3=64\n",
    "\n",
    "#Make sure the mesh is divisible by meshblock size\n",
    "# e.g. num_blocks = (512/64)*(512/64)*(512/64) = 8 x 8 x 8 = 512\n",
    "num_blocks = (nx1/mbnx1)*(nx2/mbnx2)*(nx3/mbnx3)\n",
    "\n",
    "###\n",
    "# Batch file parameters\n",
    "# num_nodes should be less than or equal to the max number of nodes in your cluster\n",
    "# num_tasks_per_node should be less than or equal to the max number of nodes in your cluster \n",
    "# e.g. 512 meshblocks / 32 core/node * 1 core/meshblock = 16 nodes -  c5n.18xlarge\n",
    "#num_nodes = 2\n",
    "\n",
    "# e.g. 64 meshblocks / 4 core/node * 1 core/meshblock = 4 nodes - c5n.2xlarge\n",
    "num_nodes = 2\n",
    "num_of_threads = 1\n",
    "\n",
    "num_tasks_per_node = num_blocks/num_nodes/num_of_threads\n",
    "cpus_per_task = num_of_threads\n",
    "\n",
    "# fake account_name \n",
    "account_name = \"12345\" \n",
    "partition = \"q2\"\n",
    "use_efa=\"YES\"\n",
    "\n",
    "#This is where the program is installed on the cluster\n",
    "exe_path = \"/shared/athena-public-version/bin/athena\"\n",
    "#This is where the program is going to run on the cluster\n",
    "work_dir = '/shared/tmp/'+job_name\n",
    "ph = { '${nx1}': str(nx1), \n",
    "       '${nx2}': str(nx2),\n",
    "       '${nx3}': str(nx3),\n",
    "       '${mbnx1}': str(mbnx1),\n",
    "       '${mbnx2}': str(mbnx2),\n",
    "       '${mbnx3}': str(mbnx3), \n",
    "       '${num_of_threads}' : str(num_of_threads)}\n",
    "template_to_file(input_file_ini, 'build/'+input_file, ph)\n",
    "\n",
    "ph = {'${nodes}': str(num_nodes),\n",
    "      '${ntasks-per-node}': str(int(num_tasks_per_node)),\n",
    "      '${cpus-per-task}': str(cpus_per_task),\n",
    "      '${account}': account_name,\n",
    "      '${partition}': partition,\n",
    "      '${job-name}': job_name,\n",
    "      '${EXE_PATH}': exe_path,\n",
    "      '${WORK_DIR}': work_dir,\n",
    "      '${input-file}': input_file,\n",
    "      '${BUCKET_NAME}': my_bucket_name,\n",
    "      '${PREFIX}': my_prefix,\n",
    "      '${USE_EFA}': use_efa,\n",
    "      '${OUTPUT_FOLDER}': \"output/\",\n",
    "      '${NUM_OF_THREADS}' : str(num_of_threads)}\n",
    "template_to_file(batch_file_ini, 'build/'+batch_file, ph)\n",
    "\n",
    "# create batch and \n",
    "def upload_athena_files(input_file, batch_file):\n",
    "    session = boto3.Session()\n",
    "    s3_client = session.client('s3')\n",
    "\n",
    "    try:\n",
    "        resp = s3_client.upload_file('build/'+input_file, my_bucket_name, my_prefix+'/'+input_file)\n",
    "        resp = s3_client.upload_file('build/'+batch_file, my_bucket_name, my_prefix+'/'+batch_file)\n",
    "    except ClientError as e:\n",
    "        print(e)\n",
    "\n",
    "# upload to S3 for use later\n",
    "upload_athena_files(input_file, batch_file)\n",
    "\n",
    "job_script = \"#!/bin/bash\\n/shared/tmp/fetch_and_run.sh {} {} {} {} {}\".format(my_bucket_name, my_prefix, input_file, batch_file, job_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "slurm_job_submit_base=slurm_rest_base+'/job/submit'\n",
    "\n",
    "job_script = \"#!/bin/bash\\n/shared/tmp/fetch_and_run.sh {} {} {} {} {}\".format(my_bucket_name,my_prefix, input_file, batch_file, job_name)\n",
    "\n",
    "#in order to use Slurm REST to submit jobs, you need to have the working directory permission set to nobody:nobody. in this case /efs/tmp\n",
    "data = {'job':{ 'account': '12345', 'partition':'q2', 'name': 'my-athena', 'current_working_directory':'/shared/tmp/', 'environment': {\"PATH\": \"/bin:/usr/bin/:/usr/local/bin/:/opt/slurm/bin:/opt/amazon/openmpi/bin\",\"LD_LIBRARY_PATH\":\n",
    "\"/lib/:/lib64/:/usr/local/lib:/opt/slurm/lib:/opt/slurm/lib64\"}}, 'script':job_script}\n",
    "\n",
    "###\n",
    "# This job submission will generate two jobs , the job_id returned in the response is for the bash job itself. the sbatch will be the job_id+1 run subsequently.\n",
    "#\n",
    "resp_job_submit = post_response_as_json(slurm_job_submit_base, data=json.dumps(data))\n",
    "\n",
    "\n",
    "print(resp_job_submit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List recent jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all the jobs immediately after the previous step. This should return two running jobs. \n",
    "slurm_jobs_base=slurm_rest_base+'/jobs'\n",
    "\n",
    "jobs = get_response_as_json(slurm_jobs_base)\n",
    "# print(jobs)\n",
    "jobs_headers = [ 'job_id', 'job_state', 'account', 'batch_host', 'nodes', 'cluster', 'partition', 'current_working_directory']\n",
    "\n",
    "# newer version of slurm \n",
    "#print_table_from_json_array(jobs_headers, jobs['jobs'])\n",
    "print_table_from_json_array(jobs_headers, jobs)\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting \n",
    "The Slurm account information is managed by slurmdbd process and stored in a data store( local file or a relational database). In our setup, we use an AWS RDS MySQL database, which has IAM authentication enabled and is running in the save VPC as the ParallelCluster. We can access the database from this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the root ca and install mysql.connector\n",
    "# only need to do this once\n",
    "#\n",
    "\n",
    "#!wget https://s3.amazonaws.com/rds-downloads/rds-ca-2019-root.pem\n",
    "#!pip install mysql.connector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6. Connect to the mysql database and query the parallel_job_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector.constants import ClientFlag\n",
    "import sys\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "ENDPOINT= rds_secret['host']\n",
    "PORT=rds_secret['port']\n",
    "USER=rds_secret['username']\n",
    "PASS=rds_secret['password']\n",
    "DBNAME=\"slurm_acct_db\"\n",
    "\n",
    "# This is the name of the cluster - default to \"parallelcluster\" - in this excercise, we default the cluster name to \"parallelcluster\"\n",
    "# If you are using sacctmgr to track accounting for multiple clusters, this could be set differently\n",
    "CLUSTERNAME=\"parallelcluster\"\n",
    "\n",
    "## use these of your want to use IAM authentication\n",
    "#os.environ['LIBMYSQL_ENABLE_CLEARTEXT_PLUGIN'] = '1\n",
    "#session = boto3.Session()\n",
    "#client = boto3.client('rds')\n",
    "#token = client.generate_db_auth_token(DBHostname=ENDPOINT, Port=PORT, DBUsername=USR, Region=REGION)\n",
    "\n",
    "config = {\n",
    "    'user': USER,\n",
    "#    'password': token,\n",
    "    'password': PASS,  \n",
    "    'host': ENDPOINT,\n",
    "    'port': PORT,\n",
    "    'database': DBNAME,\n",
    "# needed for IAM authentication\n",
    "#    'client_flags': [ClientFlag.SSL],\n",
    "#    'ssl_ca': 'rds-ca-2019-root.pem',\n",
    "}\n",
    "\n",
    "print (config)\n",
    "table_headers=['job_db_inx', 'account', 'cpus_req', 'job_name', 'id_job', 'nodelist', 'partition', 'time_submit', 'time_start', 'time_end', 'duration(s)', 'work_dir']\n",
    "table = [table_headers]\n",
    "\n",
    "try:\n",
    "    conn =  mysql.connector.connect(**config)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"SELECT job_db_inx, account, cpus_req, job_name, id_job, nodelist, 'partition', time_submit, time_start, time_end, work_dir from {}_job_table\"\"\".format(CLUSTERNAME))\n",
    "    query_results = cur.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Database connection failed due to {}\".format(e)) \n",
    "    raise\n",
    "\n",
    "#job_table_header =[(0,'job_id_inx'), (3, 'account'), ()) \n",
    "for r in query_results:\n",
    "    l = list(r)\n",
    "    # add a duration before the last element\n",
    "    l.append(l[10])\n",
    "    #duration\n",
    "    if not l[8] == 0:\n",
    "        l[10] = (l[9] - l[8])\n",
    "    else:\n",
    "        l[10] = 0\n",
    "    l[7] = get_localtime(l[7])\n",
    "    l[8] = get_localtime(l[8])\n",
    "    l[9] = get_localtime(l[9])\n",
    "    table.append(l)\n",
    "    \n",
    "\n",
    "display_table(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Athena++ Simulation Results\n",
    "In this notebook, we are going to use the python library comes with Athena++ to read and visualize the simulation results.\n",
    "\n",
    "In the previous notebook, we saved the simulation results in s3://<bucketname>/athema/$job_name/output folder\n",
    "\n",
    "Import the hdf python code that came with Athena++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import clear_output\n",
    "import h5py\n",
    "\n",
    "#Do this once. clone the athena++ source code , and the hdf5 python package we need is under vis/python folder\n",
    "\n",
    "if not os.path.isdir('athena-public-version'):\n",
    "    !git clone https://github.com/PrincetonUniversity/athena-public-version\n",
    "else:\n",
    "    print(\"Athena++ code already cloned, skip\")\n",
    "    \n",
    "sys.path.insert(0, 'athena-public-version/vis/python')\n",
    "import athena_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=job_name+'/output'\n",
    "output_folder = my_bucket_name+'/athena/'+data_folder\n",
    "\n",
    "if not os.path.isdir(job_name):\n",
    "    !mkdir -p $job_name\n",
    "    !aws s3 cp s3://$output_folder/ ./$data_folder/ --recursive\n",
    "else:\n",
    "    print('project folder exists, skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the hst data\n",
    "History data shows the overs all parameter changes over time. The time interval can be different from that of the hdf5 files.\n",
    "\n",
    "In OrszagTang simulations, the variables in the hst files are 'time', 'dt', 'mass', '1-mom', '2-mom', '3-mom', '1-KE', '2-KE', '3-KE', 'tot-E', '1-ME', '2-ME', '3-ME'\n",
    "\n",
    "All the variables a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hst = athena_read.hst(data_folder+'/OrszagTang.hst')\n",
    "\n",
    "# cannot use this reliably because hst and hdf can have different number of time steps. In this case,we have the same number of steps\n",
    "num_timesteps = len(hst['time'])\n",
    "\n",
    "print(hst.keys())\n",
    "\n",
    "plt.plot(hst['time'], hst['dt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading HDF5 data files \n",
    "\n",
    "The hdf5 data files contain all variables inside all meshblocks. There are some merging and calculating work to be done before we can visualizing the result. Fortunately ,Athena++ vis/hdf package takes care of the hard part. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's example the content of the hdf files\n",
    "\n",
    "f = h5py.File(data_folder+'/OrszagTang.out2.00001.athdf', 'r')\n",
    "# variable lists <KeysViewHDF5 ['B', 'Levels', 'LogicalLocations', 'prim', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v']>\n",
    "print(f.keys())\n",
    "\n",
    "#<HDF5 dataset \"B\": shape (3, 512, 64, 64, 64), type \"<f4\"> \n",
    "print(f['prim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation result data \n",
    "\n",
    "Raw athdf data has the following keys\n",
    "<KeysViewHDF5 ['B', 'Levels', 'LogicalLocations', 'prim', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v']>\n",
    "\n",
    "After athena_read.athdf() call, the result contains keys, which can be used as the field name\n",
    "['Coordinates', 'DatasetNames', 'MaxLevel', 'MeshBlockSize', 'NumCycles', 'NumMeshBlocks', 'NumVariables', 'RootGridSize', 'RootGridX1', 'RootGridX2', 'RootGridX3', 'Time', 'VariableNames', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v', 'rho', 'press', 'vel1', 'vel2', 'vel3', 'Bcc1', 'Bcc2', 'Bcc3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_athdf(filename, num_step):\n",
    "    print(\"Processing \", filename)\n",
    "    athdf = athena_read.athdf(filename)\n",
    "    return athdf\n",
    "\n",
    "# extract list of fields and take a slice in one dimension, dimension can be 'x', 'y', 'z'\n",
    "def read_all_timestep (data_file_name_template, num_steps, field_names, slice_number, dimension):\n",
    "\n",
    "    if not dimension in ['x', 'y', 'z']:\n",
    "        print(\"dimension can only be 'x/y/z'\")\n",
    "        return\n",
    "    \n",
    "    # would ideally process all time steps together and store themn in memory. However, they are too big, will have to trade time for memory \n",
    "    result = {}\n",
    "    for f in field_names:\n",
    "        result[f] = list()\n",
    "        \n",
    "    for i in range(num_steps):\n",
    "        fn = data_file_name_template.format(str(i).zfill(5))\n",
    "        athdf = process_athdf(fn, i)\n",
    "        for f in field_names:\n",
    "            if dimension == 'x':\n",
    "                result[f].append(athdf[f][slice_number,:,:])\n",
    "            elif dimension == 'y':\n",
    "                result[f].append(athdf[f][:, slice_number,:])\n",
    "            else:\n",
    "                result[f].append(athdf[f][:,:, slice_number])\n",
    "                        \n",
    "    return result\n",
    "\n",
    "def animate_slice(data):\n",
    "    plt.figure()\n",
    "    for i in range(len(data)):\n",
    "        plt.imshow(data[i])\n",
    "        plt.title('Frame %d' % i)\n",
    "        plt.show()\n",
    "        plt.pause(0.2)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_file_name_template = data_folder+'/OrszagTang.out2.{}.athdf'\n",
    "\n",
    "# this is time consuming, try do it once\n",
    "data = read_all_timestep(data_file_name_template, num_timesteps, ['press', 'rho'], 1, 'x')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle through the time steps and look at pressure\n",
    "animate_slice(data['press'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at density\n",
    "animate_slice(data['rho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't forget to clean up\n",
    "\n",
    "1. Delete the ParallelCluster\n",
    "2. Delete the RDS\n",
    "3. S3 bucket\n",
    "4. Secrets used in this excercise\n",
    "\n",
    "Deleting VPC is risky, I will leave it out for you to manually clean it up if you created a new VPC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pcluster delete $pcluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(workshop)\n",
    "\n",
    "\n",
    "# delete the rds database\n",
    "workshop.detele_rds_instance(region, session, db_name)\n",
    "\n",
    "#Delete the secrets\n",
    "workshop.delete_secrets_with_force(region, session, [slurm_secret_name, rds_secret_name])\n",
    "\n",
    "\n",
    "workshop.delete_bucket_completely(my_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
