{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AWS ParallelCluster\n",
    "\n",
    "This is a shortened version. The creation of the ParallelCluster is hidden in pcluster-athena.py. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import base64\n",
    "import docker\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import project_path # path to helper methods\n",
    "from lib import workshop\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "import pcluster_athena\n",
    "importlib.reload(pcluster_athena)\n",
    "\n",
    "\n",
    "# ssh key for access the pcluster. this key is not needed  in this excercise, but useful if you need to ssh into the headnode of the pcluster\n",
    "key_name = 'pcluster-athena-key'\n",
    "keypair_saved_path = './'+key_name+'.pem'\n",
    "# unique name of the pcluster\n",
    "pcluster_name = 'myTestCluster'\n",
    "# the slurm REST token is generated from the headnode and stored in Secrets Manager. This token is used in makeing REST API calls to the Slurm REST endpoint running on the headnode \n",
    "slurm_secret_name = \"slurm_token_{}\".format(pcluster_name)\n",
    "# We only need one subnet for the pcluster, but two subnets are needed for RDS instance. If use existing VPC, we will use the default VPC, and the first subnet in default VPC\n",
    "use_existing_vpc = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the parallel cluster\n",
    "In this simpler version, we will run the cluster creation in the background. The process will take around 10 minutes. The two steps that take longer than the rest are the creation of a MySQL RDS instance, and the cluster itself. \n",
    "\n",
    "If you want to see how each step is done, please use the \"pcluster-athena++\" notebook in the same directory.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the cluster - # You can rerun the rest of the notebook again with no harm. There are checks in place for existing resoources. \n",
    "\n",
    "my_bucket_name, db_name, slurm_secret_name, rds_secret_name = pcluster_athena.create_pcluster()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pcluster_status = !pcluster status $pcluster_name\n",
    "\n",
    "# get the second part of 'MasterPrivateIP: 172.16.2.92'\n",
    "slurm_host = pcluster_status.grep('MasterPrivateIP').s.split()[1]\n",
    "\n",
    "print(slurm_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate with Slurm REST API running on the head node\n",
    "\n",
    "SLURM REST is currently running on the headnode, using jwt as the auth mechanism. \n",
    "\n",
    "First from the openapi endpoint /openapi/v3, we can exam the schema of the Slurm REST API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Store the JWT token in Secrete Manager\n",
    "JWT token can be created using the \"scontrol token username=slurm\" command on the head-node. To pass it securely to this notebook, we will first create a cron job on the headnode to retrieve the token, then save it in SecreteManager with a name \"slurm_token\". The default JWT token lifespan is 1800 seconds(30 mins). Run the follow script on the head-node as a cron job to update the token every 20 mins\n",
    "\n",
    "The following steps are included in the post_install_script. You DO NOT need to run it. \n",
    "#### Step 1.  Add permission to the instance role for the head-node\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Action\": [ \n",
    "        \"secretsmanager:DescribeSecret\",\n",
    "        \"secretsmanager:CreateSecret\",\n",
    "        \"secretsmanager:UpdateSecret\"],\n",
    "    \"Resource\": [\n",
    "        \"arn:aws:secretsmanager:us-east-1:<account-id>:secret:*\"\n",
    "    ],\n",
    "    \"Effect\": \"Allow\",\n",
    "    \"Sid\": \"tokensecret\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Skip step 2 and step 3 - They are now included in the post_install script**\n",
    "\n",
    "#### Step 2. Create a script \"token_refresher.sh\" \n",
    "Assume we save the following script at /shared/token_refresher.sh \n",
    "\n",
    "``` token_refresher.sh\n",
    "#!/bin/bash\n",
    "\n",
    "REGION=us-east-1\n",
    "export $(/opt/slurm/bin/scontrol token -u slurm)\n",
    "\n",
    "aws secretsmanager describe-secret --secret-id slurm_token --region $REGION\n",
    "\n",
    "if [ $? -eq 0 ]\n",
    "then\n",
    " aws secretsmanager update-secret --secret-id slurm_token --secret-string \"$SLURM_JWT\" --region $REGION\n",
    "else\n",
    " aws secretsmanager create-secret --name slurm_token --secret-string \"$SLURM_JWT\" --region $REGION\n",
    "fi\n",
    "```\n",
    "\n",
    "#### Step 3. Add a file \"slurm-token\" in /etc/cron.d/\n",
    "\n",
    "```/etc/cron.d/slurm-token\n",
    "# Run the slurm token update every 20 minues \n",
    "SHELL=/bin/bash\n",
    "PATH=/sbin:/bin:/usr/sbin:/usr/bin\n",
    "MAILTO=root\n",
    "*/20 * * * * root /shared/token_refresher.sh                                       \n",
    "```\n",
    "\n",
    "#### Step 4. Add permission to access SecretManager for this notebook\n",
    "\n",
    "Don't forget to add secretsmanager:GetSecretValue permission to the sagemaker execution role that runs this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import base64\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "slurm_user = 'slurm'\n",
    "\n",
    "def display_table(data):\n",
    "    html = \"<table>\"\n",
    "    for row in data:\n",
    "        html += \"<tr>\"\n",
    "        for field in row:\n",
    "            html += \"<td><h4>%s</h4><td>\"%(field)\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "###\n",
    "# Retrieve the slurm_token from the SecretManager\n",
    "#\n",
    "def get_secret():\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # In this sample we only handle the specific exceptions for the 'GetSecretValue' API.\n",
    "    # See https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    # We rethrow the exception by default.\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=slurm_secret_name)\n",
    "    except ClientError as e:\n",
    "        print(\"Error\", e)\n",
    "    else:\n",
    "        # Decrypts secret using the associated KMS CMK.\n",
    "        # Depending on whether the secret is a string or binary, one of these fields will be populated.\n",
    "        if 'SecretString' in get_secret_value_response:\n",
    "            secret = get_secret_value_response['SecretString']\n",
    "            return secret\n",
    "        else:\n",
    "            decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])\n",
    "            return decoded_binary_secret\n",
    "\n",
    "###\n",
    "# Retrieve the token and inject into the header for JWT auth\n",
    "#\n",
    "def update_header_token():\n",
    "    token = get_secret()\n",
    "    post_headers = {'X-SLURM-USER-NAME':slurm_user, 'X-SLURM-USER-TOKEN': token, 'Content-type': 'application/json', 'Accept': 'application/json'}\n",
    "    get_headers = {'X-SLURM-USER-NAME':slurm_user, 'X-SLURM-USER-TOKEN': token, 'Content-type': 'application/x-www-form-urlencoded', 'Accept': 'application/json'}\n",
    "    return [post_headers, get_headers]\n",
    "\n",
    "###\n",
    "# Convert response into json\n",
    "#\n",
    "def convert_response(resp):\n",
    "    resp_str = resp.content.decode('utf-8')\n",
    "    return json.loads(resp_str)\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "# Print a json array in table format\n",
    "# input: headers [json attribute name, ... ]\n",
    "# input: a - array of json objects\n",
    "def print_table_from_json_array(headers, a):\n",
    "    # add headers as the first row.\n",
    "    t = [headers]\n",
    "    for item in a:\n",
    "        result = []\n",
    "        for h in headers:\n",
    "            result.append(item[h])\n",
    "        t.append(result)\n",
    "    display_table(t)\n",
    "\n",
    "def print_table_from_dict(headers, d):\n",
    "    result = list()\n",
    "    for k,v in d.items():\n",
    "        result.append(v)\n",
    "    print_table_from_json_array(headers, result)\n",
    "        \n",
    "\n",
    "### \n",
    "# wrapper for get\n",
    "#\n",
    "def get_response_as_json(base_url):\n",
    "    _, get_headers = update_header_token()\n",
    "    resp = requests.get(base_url, headers=get_headers)\n",
    "    if resp.status_code != 200:\n",
    "        # This means something went wrong.\n",
    "        print(\"Error\" , resp.status_code)\n",
    "\n",
    "    return convert_response(resp)\n",
    "\n",
    "### \n",
    "# wrapper for post\n",
    "#\n",
    "def post_response_as_json(base_url, data):\n",
    "    post_headers, _ = update_header_token()\n",
    "    resp = requests.post(base_url, headers=post_headers, data=data)\n",
    "    if resp.status_code != 200:\n",
    "        # This means something went wrong.\n",
    "        print(\"Error\" , resp.status_code)\n",
    "\n",
    "    return convert_response(resp)\n",
    "\n",
    "###\n",
    "# Epoch time conversion\n",
    "#\n",
    "def get_localtime(t):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t))\n",
    "\n",
    "\n",
    "# create batch and \n",
    "def upload_athena_files(input_file, batch_file):\n",
    "    session = boto3.Session()\n",
    "    s3_client = session.client('s3')\n",
    "\n",
    "    try:\n",
    "        resp = s3_client.upload_file('build/'+input_file, my_bucket_name, my_prefix+'/'+input_file)\n",
    "        resp = s3_client.upload_file('build/'+batch_file, my_bucket_name, my_prefix+'/'+batch_file)\n",
    "    except ClientError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Slurm REST API Schema\n",
    "\n",
    "If you get a \"Secrets Manager can't find the specified secret\" error message, that means the cron job to store the slurm token in a secret has not been executed yet ( scheduled to run every 20 mins). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "slurm_openapi_ep = 'http://'+slurm_host+':8082/openapi/v3'\n",
    "slurm_rest_base='http://'+slurm_host+':8082/slurm/v0.0.35'\n",
    "\n",
    "_, get_headers = update_header_token()\n",
    "\n",
    "resp_api = requests.get(slurm_openapi_ep, headers=get_headers)\n",
    "print(resp_api)\n",
    "\n",
    "if resp_api.status_code != 200:\n",
    "    # This means something went wrong.\n",
    "    print(\"Error\" , resp_api.status_code)\n",
    "\n",
    "with open('build/slurm_api.json', 'w') as outfile:\n",
    "    json.dump(resp_api.json(), outfile)\n",
    "\n",
    "print(json.dumps(resp_api.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use REST API callls to interact with ParallelCluster\n",
    "\n",
    "Then we will make direct REST API requests to retrieve the partitions in response\n",
    "\n",
    "If you get server errors, most likely\n",
    "1. Cron job - token_refresher.sh (every 20 mins) hasn't been run yet after the IAM policy is updated. You can check for the slurm_token_yourClusterName secrete in AWS Secret Manager console. \n",
    "2. login to the head-node and check the system logs of \"slurmrestd\", which is running as a service. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partition_info = [\"name\", \"nodes\", \"nodes_online\", \"total_cpus\", \"total_nodes\"]\n",
    "\n",
    "##### This works as well, \n",
    "# update header in case the token has expired\n",
    "_, get_headers = update_header_token()\n",
    "\n",
    "##### call REST API directly\n",
    "slurm_partitions_url= slurm_rest_base+'/partitions/'\n",
    "partitions = get_response_as_json(slurm_partitions_url)\n",
    "#print(partitions['partitions'])\n",
    "#20.02.4 returns a dict, not an array\n",
    "print_table_from_dict(partition_info, partitions['partitions'])\n",
    "\n",
    "# newer slurmrest return proper array\n",
    "# print_table_from_json_array(partition_info, [partitions['partitions']['q1'], partitions['partitions']['q2']] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a job\n",
    "The slurm_rest_api_client job submit function doesn't include the \"script\" parameter. We will have to use the REST API Post directly. \n",
    "\n",
    "The body of the post should be like this.  \n",
    "\n",
    "```\n",
    "{\"job\": {\"account\": \"test\", \"ntasks\": 20, \"name\": \"test18.1\", \"nodes\": [2, 4],\n",
    "\"current_working_directory\": \"/tmp/\", \"environment\": {\"PATH\": \"/bin:/usr/bin/:/usr/local/bin/\",\"LD_LIBRARY_PATH\":\n",
    "\"/lib/:/lib64/:/usr/local/lib\"} }, \"script\": \"#!/bin/bash\\necho it works\"}\n",
    "```\n",
    "When the job is submitted through REST API, it will run as the user \"slurm\". That's what the work directory \"/shared/tmp\" should be owned by \"slurm:slurm\", which is done in the post_install script. \n",
    "\n",
    "fetch_and_run.sh will fetch the sbatch script and the input file from S3 and put them in /shared/tmp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program batch script, input and output files\n",
    "\n",
    "To share the pcluster among different users and make sure users can only access their own input and output files, we will use user's ow S3 buckets for input and output files.\n",
    "\n",
    "The job will be running on the ParallelCluster under /efs/tmp (for example) through a fatch (from the S3 bucket) and run script and the output will be stored in the same bucket under \"output\" path. \n",
    "\n",
    "If the simulation results are stored in vtk files, which are merged into single block vtk files from individual mesh block vtk files. The merging process is programmed in the batch script after the simulation executions. \n",
    "\n",
    "```\n",
    "list=$(ls | grep block |grep vtk)\n",
    "\n",
    "MAXBLOCK_ID=0\n",
    "MAX_OUTSTEP=0\n",
    "for l in $list\n",
    "do\n",
    "  PROB_ID=$(echo $l |cut -d '.' -f 1)\n",
    "  OUTPUT_ID=$(echo $l| cut -d '.' -f 3 |cut --complement -c 1-3)\n",
    "\n",
    "  mb_id=$(echo $l| cut -d '.' -f 2 | cut --complement -c 1-5)\n",
    "  MAXBLOCK_ID=$(( mb_id > MAXBLOCK_ID? mb_id: MAXBLOCK_ID))\n",
    "\n",
    "  os_id=$(echo $l| cut -d '.' -f 4)\n",
    "  os_id=$((10#$os_id))\n",
    "  MAX_OUTSTEP=$(( os_id > MAX_OUTSTEP? os_id: MAX_OUTSTEP))\n",
    "done\n",
    "\n",
    "cp /shared/athena-public-version/vis/vtk/join_* .\n",
    "gcc -o join_vtk++ join_vtk++.c\n",
    "./join_all_vtk.sh $PROB_ID $OUTPUT_ID $MAXBLOCK_ID $MAX_OUTSTEP\n",
    "\n",
    "# copy the output files to S3 , excluding the block files\n",
    "aws s3 cp . s3://$BUCKET_NAME/$PREFIX/$OUTPUT_FOLDER/ --recursive --exclude \"*.block*\"\n",
    "```\n",
    "\n",
    "In this notebook, we will use hdf5 format for the output data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Where the batch script, input file, output files are uploaded to S3\n",
    "job_name = \"orszag-tang-lowres\"\n",
    "my_prefix = \"athena/\"+job_name\n",
    "\n",
    "# template files for input and batch script\n",
    "input_file_ini = \"config/athinput_orszag_tang.ini\"\n",
    "batch_file_ini = \"config/batch_athena_sh.ini\"\n",
    "\n",
    "# actual input and batch script files\n",
    "input_file = \"athinput_orszag_tang.input\"\n",
    "batch_file = \"batch_athena.sh\"\n",
    "    \n",
    "################## Begin ###################################\n",
    "# Mesh/Meshblock parameters\n",
    "# nx1,nx2,nx3 - number of zones in x,y,z\n",
    "# mbx1, mbx2, mbx3 - meshblock size \n",
    "# nx1/mbx1 X nx2/mbx2 X nx3/mbx3 = number of meshblocks - this should be the number of cores you are running the simulation on \n",
    "# e.g. mesh 100 X 100 X 100 with meshsize 50 X 50 X 50 will yield 2X2X2 = 8 blocks, run this on a cluster with 8 cores \n",
    "# \n",
    "\n",
    "#Mesh - actual domain of the problem \n",
    "# 512X512X512 cells with 64x64x64 meshblock - will have 8X8X8 = 512 meshblocks - if running on 32 cores/node, will need \n",
    "# 512/32=16 nodes\n",
    "nx1=256\n",
    "nx2=256\n",
    "nx3=256\n",
    "\n",
    "#Meshblock - each meshblock size - not too big \n",
    "mbnx1=64\n",
    "mbnx2=64\n",
    "mbnx3=64\n",
    "\n",
    "# for c5n.18xlarge without HyperThreading, the number of cores is 32 - change this accordingly. \n",
    "num_cores_per_node = 32\n",
    "num_of_threads = 1\n",
    "\n",
    "# fake account_name \n",
    "account_name = \"12345\" \n",
    "partition = \"q1\"\n",
    "\n",
    "# turn on/off EFA support in the script\n",
    "use_efa=\"YES\"\n",
    "################# END ####################################\n",
    "\n",
    "#Make sure the mesh is divisible by meshblock size\n",
    "# e.g. num_blocks = (512/64)*(512/64)*(512/64) = 8 x 8 x 8 = 512\n",
    "num_blocks = (nx1/mbnx1)*(nx2/mbnx2)*(nx3/mbnx3)\n",
    "\n",
    "###\n",
    "# Batch file parameters\n",
    "# num_nodes should be less than or equal to the max number of nodes in your cluster\n",
    "# num_tasks_per_node should be less than or equal to the max number of nodes in your cluster \n",
    "# e.g. 512 meshblocks / 32 core/node * 1 core/meshblock = 16 nodes -  c5n.18xlarge\n",
    "num_nodes = num_blocks/num_cores_per_node\n",
    "\n",
    "num_tasks_per_node = num_blocks/num_nodes/num_of_threads\n",
    "cpus_per_task = num_of_threads\n",
    "\n",
    "\n",
    "#This is where the program is installed on the cluster\n",
    "exe_path = \"/shared/athena-public-version/bin/athena\"\n",
    "#This is where the program is going to run on the cluster\n",
    "work_dir = '/shared/tmp/'+job_name\n",
    "ph = { '${nx1}': str(nx1), \n",
    "       '${nx2}': str(nx2),\n",
    "       '${nx3}': str(nx3),\n",
    "       '${mbnx1}': str(mbnx1),\n",
    "       '${mbnx2}': str(mbnx2),\n",
    "       '${mbnx3}': str(mbnx3), \n",
    "       '${num_of_threads}' : str(num_of_threads)}\n",
    "pcluster_athena.template_to_file(input_file_ini, 'build/'+input_file, ph)\n",
    "\n",
    "ph = {'${nodes}': str(num_nodes),\n",
    "      '${ntasks-per-node}': str(int(num_tasks_per_node)),\n",
    "      '${cpus-per-task}': str(cpus_per_task),\n",
    "      '${account}': account_name,\n",
    "      '${partition}': partition,\n",
    "      '${job-name}': job_name,\n",
    "      '${EXE_PATH}': exe_path,\n",
    "      '${WORK_DIR}': work_dir,\n",
    "      '${input-file}': input_file,\n",
    "      '${BUCKET_NAME}': my_bucket_name,\n",
    "      '${PREFIX}': my_prefix,\n",
    "      '${USE_EFA}': use_efa,\n",
    "      '${OUTPUT_FOLDER}': \"output/\",\n",
    "      '${NUM_OF_THREADS}' : str(num_of_threads)}\n",
    "pcluster_athena.template_to_file(batch_file_ini, 'build/'+batch_file, ph)\n",
    "\n",
    "# upload to S3 for use later\n",
    "upload_athena_files(input_file, batch_file)\n",
    "\n",
    "job_script = \"#!/bin/bash\\n/shared/tmp/fetch_and_run.sh {} {} {} {} {}\".format(my_bucket_name, my_prefix, input_file, batch_file, job_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "slurm_job_submit_base=slurm_rest_base+'/job/submit'\n",
    "\n",
    "job_script = \"#!/bin/bash\\n/shared/tmp/fetch_and_run.sh {} {} {} {} {}\".format(my_bucket_name,my_prefix, input_file, batch_file, job_name)\n",
    "\n",
    "#in order to use Slurm REST to submit jobs, you need to have the working directory permission set to nobody:nobody. in this case /efs/tmp\n",
    "data = {'job':{ 'account': account_name, 'partition': partition, 'name': 'my-athena', 'current_working_directory':'/shared/tmp/', 'environment': {\"PATH\": \"/bin:/usr/bin/:/usr/local/bin/:/opt/slurm/bin:/opt/amazon/openmpi/bin\",\"LD_LIBRARY_PATH\":\n",
    "\"/lib/:/lib64/:/usr/local/lib:/opt/slurm/lib:/opt/slurm/lib64\"}}, 'script':job_script}\n",
    "\n",
    "###\n",
    "# This job submission will generate two jobs , the job_id returned in the response is for the bash job itself. the sbatch will be the job_id+1 run subsequently.\n",
    "#\n",
    "resp_job_submit = post_response_as_json(slurm_job_submit_base, data=json.dumps(data))\n",
    "\n",
    "\n",
    "print(resp_job_submit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List recent jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all the jobs immediately after the previous step. This should return two running jobs. \n",
    "slurm_jobs_base=slurm_rest_base+'/jobs'\n",
    "\n",
    "jobs = get_response_as_json(slurm_jobs_base)\n",
    "# print(jobs)\n",
    "jobs_headers = [ 'job_id', 'job_state', 'account', 'batch_host', 'nodes', 'cluster', 'partition', 'current_working_directory']\n",
    "\n",
    "# newer version of slurm \n",
    "#print_table_from_json_array(jobs_headers, jobs['jobs'])\n",
    "print_table_from_json_array(jobs_headers, jobs)\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting \n",
    "The Slurm account information is managed by slurmdbd process and stored in a data store( local file or a relational database). In our setup, we use an AWS RDS MySQL database, which has IAM authentication enabled and is running in the save VPC as the ParallelCluster. We can access the database from this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the root ca and install mysql.connector\n",
    "# only need to do this once per kernel\n",
    "#\n",
    "\n",
    "!wget https://s3.amazonaws.com/rds-downloads/rds-ca-2019-root.pem\n",
    "!pip install mysql.connector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6. Connect to the mysql database and query the parallel_job_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector.constants import ClientFlag\n",
    "import sys\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rds_secret = json.loads(pcluster_athena.get_slurm_dbd_rds_secret(rds_secret_name))\n",
    "\n",
    "ENDPOINT= rds_secret['host']\n",
    "PORT=rds_secret['port']\n",
    "USER=rds_secret['username']\n",
    "PASS=rds_secret['password']\n",
    "DBNAME=\"slurm_acct_db\"\n",
    "\n",
    "# This is the name of the cluster - default to \"parallelcluster\" - in this excercise, we default the cluster name to \"parallelcluster\"\n",
    "# If you are using sacctmgr to track accounting for multiple clusters, this could be set differently\n",
    "CLUSTERNAME=\"parallelcluster\"\n",
    "\n",
    "config = {\n",
    "    'user': USER,\n",
    "    'password': PASS,  \n",
    "    'host': ENDPOINT,\n",
    "    'port': PORT,\n",
    "    'database': DBNAME,\n",
    "}\n",
    "\n",
    "table_headers=['job_db_inx', 'account', 'cpus_req', 'job_name', 'id_job', 'nodelist', 'partition', 'time_submit', 'time_start', 'time_end', 'duration(s)', 'work_dir']\n",
    "table = [table_headers]\n",
    "\n",
    "try:\n",
    "    conn =  mysql.connector.connect(**config)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"SELECT job_db_inx, account, cpus_req, job_name, id_job, nodelist, 'partition', time_submit, time_start, time_end, work_dir from {}_job_table\"\"\".format(CLUSTERNAME))\n",
    "    query_results = cur.fetchall()\n",
    "except Exception as e:\n",
    "    print(\"Database connection failed due to {}\".format(e)) \n",
    "    raise\n",
    "\n",
    "#job_table_header =[(0,'job_id_inx'), (3, 'account'), ()) \n",
    "for r in query_results:\n",
    "    l = list(r)\n",
    "    # add a duration before the last element\n",
    "    l.append(l[10])\n",
    "    #duration\n",
    "    if l[8] ==0 or l[9]==0:\n",
    "        l[10] = 0\n",
    "    else:\n",
    "        l[10] = (l[9] - l[8])\n",
    "\n",
    "    l[7] = get_localtime(l[7])\n",
    "    l[8] = get_localtime(l[8])\n",
    "    l[9] = get_localtime(l[9])\n",
    "    table.append(l)\n",
    "    \n",
    "\n",
    "display_table(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Athena++ Simulation Results\n",
    "In this notebook, we are going to use the python library comes with Athena++ to read and visualize the simulation results.\n",
    "\n",
    "In the previous notebook, we saved the simulation results in s3://<bucketname>/athema/$job_name/output folder\n",
    "\n",
    "Import the hdf python code that came with Athena++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import clear_output\n",
    "import h5py\n",
    "\n",
    "#Do this once. clone the athena++ source code , and the hdf5 python package we need is under vis/python folder\n",
    "\n",
    "if not os.path.isdir('athena-public-version'):\n",
    "    !git clone https://github.com/PrincetonUniversity/athena-public-version\n",
    "else:\n",
    "    print(\"Athena++ code already cloned, skip\")\n",
    "    \n",
    "sys.path.insert(0, 'athena-public-version/vis/python')\n",
    "import athena_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=job_name+'/output'\n",
    "output_folder = my_bucket_name+'/athena/'+data_folder\n",
    "\n",
    "if not os.path.isdir(job_name):\n",
    "    !mkdir -p $job_name\n",
    "    !aws s3 cp s3://$output_folder/ ./$data_folder/ --recursive\n",
    "else:\n",
    "    print('project folder exists, skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the hst data\n",
    "History data shows the overs all parameter changes over time. The time interval can be different from that of the hdf5 files.\n",
    "\n",
    "In OrszagTang simulations, the variables in the hst files are 'time', 'dt', 'mass', '1-mom', '2-mom', '3-mom', '1-KE', '2-KE', '3-KE', 'tot-E', '1-ME', '2-ME', '3-ME'\n",
    "\n",
    "All the variables a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hst = athena_read.hst(data_folder+'/OrszagTang.hst')\n",
    "\n",
    "# cannot use this reliably because hst and hdf can have different number of time steps. In this case,we have the same number of steps\n",
    "num_timesteps = len(hst['time'])\n",
    "\n",
    "print(hst.keys())\n",
    "\n",
    "plt.plot(hst['time'], hst['dt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading HDF5 data files \n",
    "\n",
    "The hdf5 data files contain all variables inside all meshblocks. There are some merging and calculating work to be done before we can visualizing the result. Fortunately ,Athena++ vis/hdf package takes care of the hard part. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's example the content of the hdf files\n",
    "\n",
    "f = h5py.File(data_folder+'/OrszagTang.out2.00001.athdf', 'r')\n",
    "# variable lists <KeysViewHDF5 ['B', 'Levels', 'LogicalLocations', 'prim', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v']>\n",
    "print(f.keys())\n",
    "\n",
    "#<HDF5 dataset \"B\": shape (3, 512, 64, 64, 64), type \"<f4\"> \n",
    "print(f['prim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation result data \n",
    "\n",
    "Raw athdf data has the following keys\n",
    "<KeysViewHDF5 ['B', 'Levels', 'LogicalLocations', 'prim', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v']>\n",
    "\n",
    "After athena_read.athdf() call, the result contains keys, which can be used as the field name\n",
    "['Coordinates', 'DatasetNames', 'MaxLevel', 'MeshBlockSize', 'NumCycles', 'NumMeshBlocks', 'NumVariables', 'RootGridSize', 'RootGridX1', 'RootGridX2', 'RootGridX3', 'Time', 'VariableNames', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v', 'rho', 'press', 'vel1', 'vel2', 'vel3', 'Bcc1', 'Bcc2', 'Bcc3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_athdf(filename, num_step):\n",
    "    print(\"Processing \", filename)\n",
    "    athdf = athena_read.athdf(filename)\n",
    "    return athdf\n",
    "\n",
    "# extract list of fields and take a slice in one dimension, dimension can be 'x', 'y', 'z'\n",
    "def read_all_timestep (data_file_name_template, num_steps, field_names, slice_number, dimension):\n",
    "\n",
    "    if not dimension in ['x', 'y', 'z']:\n",
    "        print(\"dimension can only be 'x/y/z'\")\n",
    "        return\n",
    "    \n",
    "    # would ideally process all time steps together and store themn in memory. However, they are too big, will have to trade time for memory \n",
    "    result = {}\n",
    "    for f in field_names:\n",
    "        result[f] = list()\n",
    "        \n",
    "    for i in range(num_steps):\n",
    "        fn = data_file_name_template.format(str(i).zfill(5))\n",
    "        athdf = process_athdf(fn, i)\n",
    "        for f in field_names:\n",
    "            if dimension == 'x':\n",
    "                result[f].append(athdf[f][slice_number,:,:])\n",
    "            elif dimension == 'y':\n",
    "                result[f].append(athdf[f][:, slice_number,:])\n",
    "            else:\n",
    "                result[f].append(athdf[f][:,:, slice_number])\n",
    "                        \n",
    "    return result\n",
    "\n",
    "def animate_slice(data):\n",
    "    plt.figure()\n",
    "    for i in range(len(data)):\n",
    "        plt.imshow(data[i])\n",
    "        plt.title('Frame %d' % i)\n",
    "        plt.show()\n",
    "        plt.pause(0.2)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_file_name_template = data_folder+'/OrszagTang.out2.{}.athdf'\n",
    "\n",
    "# this is time consuming, try do it once\n",
    "data = read_all_timestep(data_file_name_template, num_timesteps, ['press', 'rho'], 1, 'x')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle through the time steps and look at pressure\n",
    "animate_slice(data['press'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at density\n",
    "animate_slice(data['rho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't forget to clean up\n",
    "\n",
    "1. Delete the ParallelCluster\n",
    "2. Delete the RDS\n",
    "3. S3 bucket\n",
    "4. Secrets used in this excercise\n",
    "\n",
    "Deleting VPC is risky, I will leave it out for you to manually clean it up if you created a new VPC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pcluster delete $pcluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pcluster_athena)\n",
    "importlib.reload(workshop)\n",
    "\n",
    "db_name = 'pclusterdb'\n",
    "slurm_secret_name='slurm_token_myTestCluster'\n",
    "\n",
    "pcluster_athena.cleanup_cluster(my_bucket_name, db_name, slurm_secret_name, rds_secret_name, pcluster_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
