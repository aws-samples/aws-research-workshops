{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to AWS ParallelCluster\n",
    "\n",
    "This is a shortened version. Steps to prepare (before and after) for the ParallelCluster are coded in pcluster-athena.py. \n",
    "\n",
    "#### Before: \n",
    "- Create ssh key\n",
    "- Create or use existing default VPC\n",
    "- Create an S3 bucket for config, post install, sbatch, Athena++ initial condition files\n",
    "- Create or use exsiting MySQL RDS database for Slurm accounting\n",
    "- Update VPC security group to allow traffic to MySQL RDS\n",
    "- Create a secret for RDS in AWS Secret Manager\n",
    "- Upload post install script to S3\n",
    "- Fill the ParallelCluster config template with values of region, VPC_ID, SUBNET_ID, KEY_NAME, POST_INSTALLSCRIPT_LOCATION and POST_INSALL_SCRIPT_ARGS \n",
    "- Upload the config file to S3\n",
    "\n",
    "#### After\n",
    "- Update VPC security group attached to the headnode to open port 8082 to this notebook\n",
    "\n",
    "#### Note: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import base64\n",
    "import docker\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import project_path # path to helper methods\n",
    "from lib import workshop\n",
    "from botocore.exceptions import ClientError\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "#sys.path.insert(0, '.')\n",
    "import pcluster_athena\n",
    "importlib.reload(pcluster_athena)\n",
    "\n",
    "\n",
    "# unique name of the pcluster\n",
    "pcluster_name = 'myPC5c'\n",
    "config_name = \"config\"\n",
    "post_install_script_prefix = \"scripts/pcluster_post_install.sh\"\n",
    "\n",
    "# Graviton cluster\n",
    "#pcluster_name = 'myPC6g'\n",
    "#config_name=\"config-c6g\"\n",
    "#post_install_script_prefix = \"scripts/pcluster_post_install-6g.sh\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used during developemnt, to reload the module after a change in the module\n",
    "try:\n",
    "    del sys.modules['pcluster_athena']\n",
    "except:\n",
    "    #ignore if the module is not loaded\n",
    "    print('Module not loaded, ignore')\n",
    "    \n",
    "from pcluster_athena import PClusterHelper\n",
    "# create the cluster - # You can rerun the rest of the notebook again with no harm. There are checks in place for existing resoources. \n",
    "pcluster_helper = PClusterHelper(pcluster_name, config_name, post_install_script_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the parallel cluster\n",
    "In this shortened version, we will run the cluster creation in the background, using the PClusterHelper class. \n",
    "\n",
    "The process will take up to 30 minutes. The two steps that take longer than the rest are the creation of a MySQL RDS instance, and the cluster itself. \n",
    "\n",
    "If you want to see how each step is done, please use the \"pcluster-athena++\" notebook in the same directory.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pcluster_helper.create_before()\n",
    "\n",
    "# can not properly capture the status output of the command line in the notebook from within the helper class. \n",
    "# So broke up the creation into before , create and after\n",
    "# this process will take up to 30 minutes - libhdf takes a long time to compile and install\n",
    "!pcluster create $pcluster_helper.pcluster_name -nr -c build/$config_name\n",
    "\n",
    "pcluster_helper.create_after()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pcluster_status = !pcluster status $pcluster_name\n",
    "\n",
    "# Grab the IP of the head node, on where Slurm REST endpoint runs. The returned IP is a private IP of the head node. Make sure your SageMaker notebook\n",
    "# is created in the same VPC (default VPC)\n",
    "slurm_host = pcluster_status.grep('MasterPrivateIP').s.split()[1]\n",
    "\n",
    "print(slurm_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate with Slurm REST API running on the head node\n",
    "\n",
    "SLURM REST is currently running on the headnode. The JWT token is stored in AWS Secret Manager from the head node. You will need that JWT token in the header of all your REST API requests. \n",
    "\n",
    "Don't forget to add secretsmanager:GetSecretValue permission to the sagemaker execution role that runs this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Slurm REST API Schema\n",
    "\n",
    "We will start by examing the Slurm REST API schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "slurm_openapi_ep = 'http://'+slurm_host+':8082/openapi/v3'\n",
    "slurm_rest_base='http://'+slurm_host+':8082/slurm/v0.0.35'\n",
    "\n",
    "_, get_headers = pcluster_helper.update_header_token()\n",
    "\n",
    "resp_api = requests.get(slurm_openapi_ep, headers=get_headers)\n",
    "print(resp_api)\n",
    "\n",
    "if resp_api.status_code != 200:\n",
    "    # This means something went wrong.\n",
    "    print(\"Error\" , resp_api.status_code)\n",
    "\n",
    "with open('build/slurm_api.json', 'w') as outfile:\n",
    "    json.dump(resp_api.json(), outfile)\n",
    "\n",
    "print(json.dumps(resp_api.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use REST API callls to interact with ParallelCluster\n",
    "\n",
    "Then we will make direct REST API requests to retrieve the partitions in response\n",
    "\n",
    "If you get server errors, you can login to the head-node and check the system logs of \"slurmrestd\", which is running as a service. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partition_info = [\"name\", \"nodes\", \"nodes_online\", \"total_cpus\", \"total_nodes\"]\n",
    "\n",
    "##### This works as well, \n",
    "# update header in case the token has expired\n",
    "_, get_headers = pcluster_helper.update_header_token()\n",
    "\n",
    "##### call REST API directly\n",
    "slurm_partitions_url= slurm_rest_base+'/partitions/'\n",
    "partitions = pcluster_helper.get_response_as_json(slurm_partitions_url)\n",
    "#print(partitions['partitions'])\n",
    "#20.02.4 returns a dict, not an array\n",
    "pcluster_helper.print_table_from_dict(partition_info, partitions['partitions'])\n",
    "\n",
    "# newer slurmrest return proper array\n",
    "# print_table_from_json_array(partition_info, [partitions['partitions']['q1'], partitions['partitions']['q2']] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a job\n",
    "The slurm_rest_api_client job submit function doesn't include the \"script\" parameter. We will have to use the REST API Post directly. \n",
    "\n",
    "The body of the post should be like this.  \n",
    "\n",
    "```\n",
    "{\"job\": {\"account\": \"test\", \"ntasks\": 20, \"name\": \"test18.1\", \"nodes\": [2, 4],\n",
    "\"current_working_directory\": \"/tmp/\", \"environment\": {\"PATH\": \"/bin:/usr/bin/:/usr/local/bin/\",\"LD_LIBRARY_PATH\":\n",
    "\"/lib/:/lib64/:/usr/local/lib\"} }, \"script\": \"#!/bin/bash\\necho it works\"}\n",
    "```\n",
    "When the job is submitted through REST API, it will run as the user \"slurm\". That's what the work directory \"/shared/tmp\" should be owned by \"slurm:slurm\", which is done in the post_install script. \n",
    "\n",
    "fetch_and_run.sh will fetch the sbatch script and the input file from S3 and put them in /shared/tmp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program batch script, input and output files\n",
    "\n",
    "To share the pcluster among different users and make sure users can only access their own input and output files, we will use user's ow S3 buckets for input and output files.\n",
    "\n",
    "The job will be running on the ParallelCluster under /efs/tmp (for example) through a fatch (from the S3 bucket) and run script and the output will be stored in the same bucket under \"output\" path. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################## Change these \n",
    "# Where the batch script, input file, output files are uploaded to S3\n",
    "#cluster_instance_type = 'c6gn.2xlarge'\n",
    "#simulation_name = \"orz-512x512\"\n",
    "\n",
    "cluster_instance_type = 'c5n.2xlarge'\n",
    "simulation_name = \"orz-512x512\"\n",
    "\n",
    "# for c5n.18xlarge without HyperThreading, the number of cores is 32 - change this accordingly. \n",
    "#num_cores_per_node = 32\n",
    "#partition = \"q1\"\n",
    "# for c5n.2xlarge without HyperThreading, the number of cores is 4 - change this accordingly. \n",
    "num_cores_per_node = 4\n",
    "partition = \"q3\"\n",
    "# for c5n.2xlarge wit HyperThreading, the number of cores is 4 - change this accordingly. \n",
    "#num_cores_per_node = 8\n",
    "#partition = \"q2\"\n",
    "\n",
    "\n",
    "job_name = f\"{simulation_name}-{cluster_instance_type}\"\n",
    "# fake account_name \n",
    "account_name = f\"dept-2d\" \n",
    "\n",
    "\n",
    "# turn on/off EFA support in the script\n",
    "use_efa=\"NO\"\n",
    "################# END change\n",
    "\n",
    "# prefix in S3\n",
    "my_prefix = \"athena/\"+job_name\n",
    "\n",
    "# template files for input and batch script\n",
    "input_file_ini = \"config/athinput_orszag_tang.ini\"\n",
    "batch_file_ini = \"config/batch_athena_sh.ini\"\n",
    "\n",
    "# actual input and batch script files\n",
    "input_file = \"athinput_orszag_tang.input\"\n",
    "batch_file = \"batch_athena.sh\"\n",
    "    \n",
    "################## Begin ###################################\n",
    "# Mesh/Meshblock parameters\n",
    "# nx1,nx2,nx3 - number of zones in x,y,z\n",
    "# mbx1, mbx2, mbx3 - meshblock size \n",
    "# nx1/mbx1 X nx2/mbx2 X nx3/mbx3 = number of meshblocks - this should be the number of cores you are running the simulation on \n",
    "# e.g. mesh 100 X 100 X 100 with meshsize 50 X 50 X 50 will yield 2X2X2 = 8 blocks, run this on a cluster with 8 cores \n",
    "# \n",
    "\n",
    "#Mesh - actual domain of the problem \n",
    "# 512X512X512 cells with 64x64x64 meshblock - will have 8X8X8 = 512 meshblocks - if running on 32 cores/node, will need \n",
    "# 512/32=16 nodes\n",
    "nx1=512\n",
    "nx2=512\n",
    "nx3=1\n",
    "\n",
    "#Meshblock - each meshblock size - not too big \n",
    "mbnx1=64\n",
    "mbnx2=64\n",
    "mbnx3=1\n",
    "\n",
    "\n",
    "num_of_threads = 1\n",
    "\n",
    "################# END ####################################\n",
    "\n",
    "#Make sure the mesh is divisible by meshblock size\n",
    "# e.g. num_blocks = (512/64)*(512/64)*(512/64) = 8 x 8 x 8 = 512\n",
    "num_blocks = (nx1/mbnx1)*(nx2/mbnx2)*(nx3/mbnx3)\n",
    "\n",
    "###\n",
    "# Batch file parameters\n",
    "# num_nodes should be less than or equal to the max number of nodes in your cluster\n",
    "# num_tasks_per_node should be less than or equal to the max number of nodes in your cluster \n",
    "# e.g. 512 meshblocks / 32 core/node * 1 core/meshblock = 16 nodes -  c5n.18xlarge\n",
    "num_nodes = int(num_blocks/num_cores_per_node)\n",
    "\n",
    "num_tasks_per_node = num_blocks/num_nodes/num_of_threads\n",
    "cpus_per_task = num_of_threads\n",
    "\n",
    "\n",
    "#This is where the program is installed on the cluster\n",
    "exe_path = \"/shared/athena-public-version/bin/athena\"\n",
    "#This is where the program is going to run on the cluster\n",
    "work_dir = '/shared/tmp/'+job_name\n",
    "ph = { '${nx1}': str(nx1), \n",
    "       '${nx2}': str(nx2),\n",
    "       '${nx3}': str(nx3),\n",
    "       '${mbnx1}': str(mbnx1),\n",
    "       '${mbnx2}': str(mbnx2),\n",
    "       '${mbnx3}': str(mbnx3), \n",
    "       '${num_of_threads}' : str(num_of_threads)}\n",
    "pcluster_helper.template_to_file(input_file_ini, 'build/'+input_file, ph)\n",
    "\n",
    "ph = {'${nodes}': str(num_nodes),\n",
    "      '${ntasks-per-node}': str(int(num_tasks_per_node)),\n",
    "      '${cpus-per-task}': str(cpus_per_task),\n",
    "      '${account}': account_name,\n",
    "      '${partition}': partition,\n",
    "      '${job-name}': job_name,\n",
    "      '${EXE_PATH}': exe_path,\n",
    "      '${WORK_DIR}': work_dir,\n",
    "      '${input-file}': input_file,\n",
    "      '${BUCKET_NAME}': pcluster_helper.my_bucket_name,\n",
    "      '${PREFIX}': my_prefix,\n",
    "      '${USE_EFA}': use_efa,\n",
    "      '${OUTPUT_FOLDER}': \"output/\",\n",
    "      '${NUM_OF_THREADS}' : str(num_of_threads)}\n",
    "pcluster_helper.template_to_file(batch_file_ini, 'build/'+batch_file, ph)\n",
    "\n",
    "# upload to S3 for use later\n",
    "pcluster_helper.upload_athena_files(input_file, batch_file, my_prefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_script = \"#!/bin/bash\\n/shared/tmp/fetch_and_run.sh {} {} {} {} {}\".format(pcluster_helper.my_bucket_name, my_prefix, input_file, batch_file, job_name)\n",
    "\n",
    "slurm_job_submit_base=slurm_rest_base+'/job/submit'\n",
    "\n",
    "#in order to use Slurm REST to submit jobs, you need to have the working directory permission set to nobody:nobody. in this case /efs/tmp\n",
    "data = {'job':{ 'account': account_name, 'partition': partition, 'name': job_name, 'current_working_directory':'/shared/tmp/', 'environment': {\"PATH\": \"/bin:/usr/bin/:/usr/local/bin/:/opt/slurm/bin:/opt/amazon/openmpi/bin\",\"LD_LIBRARY_PATH\":\n",
    "\"/lib/:/lib64/:/usr/local/lib:/opt/slurm/lib:/opt/slurm/lib64\"}}, 'script':job_script}\n",
    "\n",
    "###\n",
    "# This job submission will generate two jobs , the job_id returned in the response is for the bash job itself. the sbatch will be the job_id+1 run subsequently.\n",
    "#\n",
    "resp_job_submit = pcluster_helper.post_response_as_json(slurm_job_submit_base, data=json.dumps(data))\n",
    "\n",
    "\n",
    "print(resp_job_submit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List recent jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all the jobs immediately after the previous step. This should return two running jobs. \n",
    "slurm_jobs_base=slurm_rest_base+'/jobs'\n",
    "\n",
    "jobs = pcluster_helper.get_response_as_json(slurm_jobs_base)\n",
    "# print(jobs)\n",
    "jobs_headers = [ 'job_id', 'job_state', 'account', 'batch_host', 'nodes', 'cluster', 'partition', 'current_working_directory']\n",
    "\n",
    "# newer version of slurm \n",
    "#print_table_from_json_array(jobs_headers, jobs['jobs'])\n",
    "pcluster_helper.print_table_from_json_array(jobs_headers, jobs)\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mediu resolution simulation will run about ten minutes, plus the time for the cluster to spin up. Wait till the job finishes running then move to the next sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Athena++ Simulation Results\n",
    "Now we are going to use the python library comes with Athena++ to read and visualize the simulation results. Simulation data was saved in s3://<bucketname>/athema/$job_name/output folder. \n",
    "\n",
    "Import the hdf python code that came with Athena++ and copy the data to local file system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import clear_output\n",
    "import h5py\n",
    "\n",
    "#Do this once. clone the athena++ source code , and the hdf5 python package we need is under vis/python folder\n",
    "\n",
    "if not os.path.isdir('athena-public-version'):\n",
    "    !git clone https://github.com/PrincetonUniversity/athena-public-version\n",
    "else:\n",
    "    print(\"Athena++ code already cloned, skip\")\n",
    "    \n",
    "sys.path.insert(0, 'athena-public-version/vis/python')\n",
    "import athena_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=job_name+'/output'\n",
    "output_folder = pcluster_helper.my_bucket_name+'/athena/'+data_folder\n",
    "\n",
    "if not os.path.isdir(job_name):\n",
    "    !mkdir -p $job_name\n",
    "else:\n",
    "    !rm -rf $job_name/*\n",
    "\n",
    "!aws s3 cp s3://$output_folder/ ./$data_folder/ --recursive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the hst data\n",
    "History data shows the overs all parameter changes over time. The time interval can be different from that of the hdf5 files.\n",
    "\n",
    "In OrszagTang simulations, the variables in the hst files are 'time', 'dt', 'mass', '1-mom', '2-mom', '3-mom', '1-KE', '2-KE', '3-KE', 'tot-E', '1-ME', '2-ME', '3-ME'\n",
    "\n",
    "All the variables a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hst = athena_read.hst(data_folder+'/OrszagTang.hst')\n",
    "\n",
    "# cannot use this reliably because hst and hdf can have different number of time steps. In this case,we have the same number of steps\n",
    "num_timesteps = len(hst['time'])\n",
    "\n",
    "print(hst.keys())\n",
    "\n",
    "plt.plot(hst['time'], hst['dt'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading HDF5 data files \n",
    "\n",
    "The hdf5 data files contain all variables inside all meshblocks. There are some merging and calculating work to be done before we can visualizing the result. Fortunately ,Athena++ vis/hdf package takes care of the hard part. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's example the content of the hdf files\n",
    "\n",
    "f = h5py.File(data_folder+'/OrszagTang.out2.00001.athdf', 'r')\n",
    "# variable lists <KeysViewHDF5 ['B', 'Levels', 'LogicalLocations', 'prim', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v']>\n",
    "print(f.keys())\n",
    "\n",
    "#<HDF5 dataset \"B\": shape (3, 512, 64, 64, 64), type \"<f4\"> \n",
    "print(f['prim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation result data \n",
    "\n",
    "Raw athdf data has the following keys\n",
    "<KeysViewHDF5 ['B', 'Levels', 'LogicalLocations', 'prim', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v']>\n",
    "\n",
    "After athena_read.athdf() call, the result contains keys, which can be used as the field name\n",
    "['Coordinates', 'DatasetNames', 'MaxLevel', 'MeshBlockSize', 'NumCycles', 'NumMeshBlocks', 'NumVariables', 'RootGridSize', 'RootGridX1', 'RootGridX2', 'RootGridX3', 'Time', 'VariableNames', 'x1f', 'x1v', 'x2f', 'x2v', 'x3f', 'x3v', 'rho', 'press', 'vel1', 'vel2', 'vel3', 'Bcc1', 'Bcc2', 'Bcc3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_athdf(filename, num_step):\n",
    "    print(\"Processing \", filename)\n",
    "    athdf = athena_read.athdf(filename)\n",
    "    return athdf\n",
    "\n",
    "# extract list of fields and take a slice in one dimension, dimension can be 'x', 'y', 'z'\n",
    "def read_all_timestep (data_file_name_template, num_steps, field_names, slice_number, dimension):\n",
    "\n",
    "    if not dimension in ['x', 'y', 'z']:\n",
    "        print(\"dimension can only be 'x/y/z'\")\n",
    "        return\n",
    "    \n",
    "    # would ideally process all time steps together and store themn in memory. However, they are too big, will have to trade time for memory \n",
    "    result = {}\n",
    "    for f in field_names:\n",
    "        result[f] = list()\n",
    "        \n",
    "    for i in range(num_steps):\n",
    "        fn = data_file_name_template.format(str(i).zfill(5))\n",
    "        athdf = process_athdf(fn, i)\n",
    "        for f in field_names:\n",
    "            if dimension == 'x':\n",
    "                result[f].append(athdf[f][slice_number,:,:])\n",
    "            elif dimension == 'y':\n",
    "                result[f].append(athdf[f][:, slice_number,:])\n",
    "            else:\n",
    "                result[f].append(athdf[f][:,:, slice_number])\n",
    "                        \n",
    "    return result\n",
    "\n",
    "def animate_slice(data):\n",
    "    plt.figure()\n",
    "    for i in range(len(data)):\n",
    "        plt.imshow(data[i])\n",
    "        plt.title('Frame %d' % i)\n",
    "        plt.show()\n",
    "        plt.pause(0.2)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_file_name_template = data_folder+'/OrszagTang.out2.{}.athdf'\n",
    "\n",
    "# this is time consuming, try do it once\n",
    "data = read_all_timestep(data_file_name_template, num_timesteps, ['press', 'rho'], 0, 'x')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle through the time steps and look at pressure\n",
    "animate_slice(data['press'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at density\n",
    "animate_slice(data['rho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slurm accounting\n",
    "\n",
    "Slurm accounting has been moved to pcluster_cost_estimate notebook in the same folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't forget to clean up\n",
    "\n",
    "1. Delete the ParallelCluster\n",
    "2. Delete the RDS\n",
    "3. S3 bucket\n",
    "4. Secrets used in this excercise\n",
    "\n",
    "Deleting VPC is risky, I will leave it out for you to manually clean it up if you created a new VPC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pcluster_name, config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used during developemnt, to reload the module after a change in the module\n",
    "#try:\n",
    "#    del sys.modules['pcluster_athena']\n",
    "#except:\n",
    "#    #ignore if the module is not loaded\n",
    "#    print('Module not loaded, ignore')\n",
    "    \n",
    "#from pcluster_athena import PClusterHelper\n",
    "# create the cluster - # You can rerun the rest of the notebook again with no harm. There are checks in place for existing resoources. \n",
    "pcluster_helper = PClusterHelper(pcluster_name, config_name, post_install_script_prefix)\n",
    "\n",
    "\n",
    "!pcluster delete $pcluster_helper.pcluster_name\n",
    "\n",
    "pcluster_helper.cleanup_after(KeepRDS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
